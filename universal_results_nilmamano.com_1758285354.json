{
  "site": "https://nilmamano.com/blog/category/dsa",
  "items": [
    {
      "title": "BCtCI Free Resources",
      "content": "A comprehensive list of all free resources available from Beyond Cracking the Coding Interview, including chapters, tools, templates, and guides.",
      "content_type": "podcast_transcript",
      "source_url": "https://nilmamano.com/blog/bctci-free-resources?category=dsa"
    },
    {
      "title": "BCtCI Chapter: Set & Map Implementations",
      "content": "# BCtCI Chapter: Set & Map Implementations\n\nAugust 5, 2025\n![BCtCI Chapter: Set & Map Implementations](/blog/set-and-map-implementations/fig3.png)\nThis is an online chapter from Beyond Cracking the Coding\nInterview, co-authored with Gayle Laakmann McDowell, Mike Mroczka, and Aline Lerner.\nYou can try all the problems from this chapter with our AI interviewer at\nbctci.co/set-and-map-implementations.\nYou can also find full solutions in \\*\\*Python, C++, Java, and JS\\*\\* there.\nThis is a Tier 3 DS&A topic. The book contains all the Tier 1 & 2 topics. You can find other Tier 3 topics online at bctci.co (Part VIII of the ToC).\nReport bugs or typos at bctci.co/errata.\n\\*\\*Prerequisites:\\*\\* Dynamic Arrays, Sets & Maps. It is recommended to review those chapters from BCtCI before reading this one.\n# Introduction\n\nWe saw how to \\*use\\* sets and maps in the 'Sets & Maps' chapter (bctci.co/sets-and-maps). We saw that they often come in handy in coding interviews, and we discussed how to use them effectively.\nSets and maps are implemented in two main ways:\n\n- Using hash tables.\n\n- Using self-balancing binary search trees.\nWe can distinguish between the implementations by calling one \\*HashSet\\* and \\*HashMap\\* and the other \\*TreeSet\\* and \\*TreeMap\\*. They have different use cases in advanced scenarios, but for the basic APIs we'll define, the hash-table-based implementations are more common, as they have better asymptotic runtimes and are faster in practice.\nMost programming languages provide built-in hash sets and hash maps. In this chapter, we will look at how they are implemented. This will allow us to learn about the important concepts of hashing and hash tables. See the 'Self-Balancing BSTs' section in the Trees chapter for a discussion on tree-based implementations.\nWe'll start with sets, which are a bit simpler than maps.\n# Hash Set Implementation\n\nThe set API is simple:\n```\nadd(x): if x is not in the set, adds x to the set\nremove(x): if x is in the set, removes it from the set\ncontains(x): returns whether the element x is in the set\nsize(): returns the number of elements in the set\n```\nA question for the reader: Why do sets not have a function like `set(i, x)`, which sets an element at a particular index?1\nWe will build upon the concepts of dynamic arrays from the 'Dynamic Arrays' chapter (see bctci.co/implement-dynamic-array) to show how to implement sets very efficiently, which is what makes sets a versatile data structure for coding interviews.\n## Naive Set Implementation\n\nLet's start with a naive implementation idea and then identify its weaknesses so we can improve it—like we often do in interviews. What if we simply store all the set elements in a dynamic array? The issue with this idea is that checking if an element is in the set would require a linear scan, so the `contains()` method would not be very efficient.\nBy comparison, dynamic arrays do lookups \\*by index\\*, which means that we can jump directly to the correct memory address based on the given index. We would like to be able to do something similar for sets, where, just from an element `x`, we can easily find where we put it in the dynamic array.\nHere is a promising idea in this direction, though still unpolished. Say you have a set of integers. Instead of putting all the elements in a single dynamic array, initialize your set data structure with, say, `1000` dynamic arrays, and then store each integer `x` in the array at index `x % 1000`. In this way, we no longer need to scan through a list of every element each time: if the user calls `contains(1992)`, we only need to check the array at index `1992 % 1000 = 992`. That is, we only need to scan through integers that have the same remainder when divided by `1000`.\nThis crude idea could work well in certain situations, but it has two critical shortcomings that we still have to address.\n\n- The first shortcoming has to do with the \\*\\*distribution\\*\\* of the integers in the set. Consider a situation where someone uses a set to store file sizes in bytes, and in their particular application all the file sizes happened to be in whole kilobytes. Every element stored in the set, like `1000`, `2000`, or `8000`, would end up in the array at index `0`. If all the elements end up in the same index, we are back at the starting point, having to do linear scans through all the elements.\n\n- The second shortcoming is that even if the elements were perfectly distributed among the `1000` arrays, fixing the number of arrays to `1000` is too rigid. It might be overkill if we have only a few elements, and it might not be enough if we store a really large number of them.\nIn the next section, we will use hash functions to address the first shortcoming. For the second shortcoming, we will see a familiar technique from dynamic arrays: dynamic resizing.\n## Hash Functions\n\nA \\*hash function\\* is a function that maps inputs of some type, such as numbers or strings, into a \"random-looking\" number in a predetermined output range. For instance, if `h(x)` is a hash function for integers with an output range of `0` to `999`, then it could map `1` to `111`, `2` to `832`, and `3` to itself. Even though the mapping seems \"random,\" it is important that each input always maps to the same value. That is, `h(1)` must always be `111`. We say that `111` is the \\*hash\\* of `1`.\nIn an actual hash function implementation, hashes are not truly random, but we want them to be \"random-looking\" because \\*\\*we consider a hash function \\*good\\* if it spreads the input values evenly across the output range, even if the distribution of input values is skewed in some way or follows some non-random pattern.\\*\\* For instance, `h(x) = x % 1000` is \\*not\\* a good hash function because, even though each number from `0` to `999` is the hash of the same fraction of input values, we saw that we could get a very uneven distribution for certain input patterns—like if every input was a multiple of `1000`. The challenge, then, is to spread the inputs evenly for \\*any\\* input pattern.\n### A simple hash function for integers\n\nThere are many interesting ways to hash integers into a fixed range such as `[0, 999]`. In this section, we will show a simple one known as \\*multiplicative hashing\\*. It may not have the best distribution properties, but it suffices to understand the concept and it runs in constant time.\nWe'll use `m` to denote the upper bound of the output range, which is excluded. In our example, `m = 1000`. Our hash function is based on the following observation: if we multiply an input integer by a constant between `0` and `1` with \"random-looking\" decimals such as `0.6180339887`, we get a number with a \"random-looking\" fractional part. For instance,\n\n- `1 \\* 0.618... = 0.618...`\n\n- `2 \\* 0.618... = 1.236...`\n\n- `3 \\* 0.618... = 1.854...`\nWe can then take the fractional part alone, which is between `0` and `1`, and multiply it by `m` to scale it up to the range between `0` and `m`. Finally, we can truncate the decimal part and output the integer part of this number, which will be a \"random-looking\" number between `0` and `m-1`.2\n![The steps of multiplicative hashing for an output range of [0, 999], using 0.6180339887 as the constant C](/blog/set-and-map-implementations/fig1.png)\n\\*\\*Figure 1.\\*\\* The steps of multiplicative hashing for an output range of [0, 999], using 0.6180339887 as the constant C.\n```\nC = 0.6180339887 # A \"random-looking\" fraction between 0 and 1.\n\ndef h(x, m):\nx \\*= C # A product with \"random-looking\" decimals.\n\nx -= int(x) # Keep only the fractional part.\n\nx \\*= m # Scale the number in [0, 1) up to [0, m).\n\nreturn int(x) # Return the integer part.\n\n```\nWhen `m = 1000`, the function amounts to returning the first `3` decimals of `x \\* C` as our output hash between `000` and `999`. For example:\n\n- `h(1, 1000) = 618` because `1 \\* 0.618... = 0.618...`\n\n- `h(2, 1000) = 236` because `2 \\* 0.618... = 1.236...`\n\n- `h(1000, 1000) = 33` because `1000 \\* 0.618... = 618.033...`\nThere will be cases where different inputs have the same hash. Those are called \\*collisions\\*. For instance, `2` collides with `612`:\n\n- `h(612, 1000) = 236` because `612 \\* 0.618... = 378.236...`\n\\*\\*If the number of possible inputs is larger than the output range, collisions are unavoidable.\\*\\*3\n### Hash functions in sets\n\nIf you think about it, in our naive set implementation, we used a sort of hash function to map inputs to one of our `1000` dynamic arrays: `h(x, 1000) = x % 1000`. However, as we saw, this hash function could result in a ton of collisions for certain input patterns. If we had used multiplicative hashing instead, we would have distributed the inputs much better. We'll see how it all comes together at the end of the section.\n### Hashing strings\n\nWe want to be able to hash different types, not just integers. Strings are a type we often need to hash, but this poses the extra challenge of different input and output types: string `->` number.\nWe'll follow a two-step approach:\n1. Convert the string into a number,\n2. Once we have a number, use the modulo operator (`%`) to get a remainder that fits in the output range. For instance, if the output range is `0` to `999`, we would set `m = 1000` and do:\n```\ndef h(str, m):\nnum = str\\_to\\_num(str)\nreturn num % m\n```\nIn Step (2), we \\*could\\* apply integer hashing (like multiplicative hashing) to the number returned in Step (1), but if the numbers returned by Step (1) are unique enough, the modulo operation is usually enough.\nSo, how can we convert strings into numbers? Since characters are encoded in binary, we can treat each character as a number. For instance, in Python, `ord(c)` returns the numerical code of character `c`. For this section, we will assume that we are dealing with ASCII strings, where the most common characters, like letters, numbers, and symbols, are encoded as 7-bit strings (numbers from `0` to `127`).4\nCan we use ASCII codes of the individual characters to convert a whole string into a single number?\n\\*\\*Problem set.\\*\\* Each of the following proposed implementations for the `str\\_to\\_num()` method, which converts strings to integers, is not ideal. Do you see why? Can you identify which input strings will result in \\*collisions\\* (the same output number)?\n1. Return the ASCII code of the first character in the string.\n2. Return the ASCII code of the first character in the string multiplied by the length of the string.\n3. Return the sum of the ASCII code of each character.\n4. Return the sum of the ASCII code of each character multiplied by the character's index in the string (starting from `1`).\n\\*\\*Solution.\\*\\*\n1. This function is bad because any two strings starting with the same character, like `\"Mroczka\"` and `\"McDowell\"`, end up with the same number: `ASCII(M) = 77`. Another way to see that it is bad is that it can only output `128` possible different values, which is too small to avoid collisions.\n2. This function is less bad because, at least, it can output a much larger set of numbers. However, there can still be many collisions. Any two strings starting with the same character and with the same length get the same number, like `\"aa\"` and `\"az\"`: `97 \\* 2 = 194`.\n3. This function is much better because it factors in information from all the characters. Its weakness is that the \\*order\\* of the characters does not matter: the string \"spot\" maps to the number `115 + 112 + 111 + 116 = 454`, but so do other words like `\"stop\"`, `\"pots\"`, and `\"tops\"`.\n4. This function is even better and results in fewer collisions because it uses information from all the characters \\*and\\* also uses the order. However, it is still possible to find collisions, like `\"!!\"` and `\"c\"`. The ASCII code for `'!'` is `33`, so the number for `\"!!\"` is `33 \\* 1 + 33 \\* 2 = 99`, and the ASCII code for `'c'` is also `99`.\nIt turns out that \\*\\*there is a way to map ASCII strings to numbers without ever having any collisions\\*\\*. The key is to think of the whole string as a number where each character is a 'digit' in base `128` (see Figure 2). For instance, `\"a\"` corresponds to the number `97` because that's the ASCII code for `'a'`, and `\"aa\"` corresponds to `12513` because `(97 \\* 128^1) + (97 \\* 128^0) = 12513`.\n![Visualizing a decimal number like 5213 as a sum of consecutive powers of 10, and an ASCII string like 'spot' as a sum of consecutive powers of 128](/blog/set-and-map-implementations/fig2.png)\n\\*\\*Figure 2.\\*\\* We can visualize a decimal number like 5213 as a sum of consecutive powers of 10. Likewise, we can visualize an ASCII string like 'spot' as a sum of consecutive powers of 128. This produces a unique numeric value for every ASCII string.\n```\ndef str\\_to\\_num(str):\nres = 0\nfor c in str:\nres = res \\* 128 + ord(c)\nreturn res\n```\nThe cost to pay for having unique numbers is that we quickly reach huge numbers, as shown in Figure 2. In fact, the numbers could easily result in overflow when using 32-bit or 64-bit integers (`128^10 > 2^64`, so any string of length `11` or more would \\*definitely\\* overflow with 64-bit integers).\nTo prevent this, we add a modulo operation to keep numbers in a manageable range. For instance, if we want the number to fit within a 32-bit integer, we can divide our number by a \\*\\*large prime\\*\\* that fits in `32` bits like `10^9 + 7` and keep only the remainder.5\n```\nLARGE\\_PRIME = 10\\*\\*9 + 7\ndef str\\_to\\_num(str):\nres = 0\nfor c in str:\nres = (res \\* 128 + ord(c)) % LARGE\\_PRIME\nreturn res\n```\nWe want `LARGE\\_PRIME` to be large to maximize the range of possible remainders and have fewer collisions. We want it to be prime because non-prime numbers can lead to more collisions for specific input distributions. For instance, when dividing numbers by `12`, which has `3` as a factor, every multiple of `3` has a remainder of `0`, `3`, `6`, or `9`. That means that if our input distribution consisted of only multiples of `3`, we would have a lot of collisions. Prime numbers like `11` do not have this issue except for multiples of `11` itself.6\nThis is a good way to map strings into integers without overflowing. The runtime is proportional to the string's length. In the 'Rolling Hash Algorithm' section, we will see another application of this method.\nAside\nHash functions have many applications beyond hash sets and maps. For instance,\nthey are used to guarantee file integrity when sending files over the network.\nThe sender computes a 64-bit hash of the entire file and sends it along with\nthe file. The recipient can then compute the hash of the received file and\ncompare it with the received hash. If the two hashes match, it means that, in\nall likelihood, the file was not corrupted on the way (with a good hash\nfunction and an output range of `2^64` different hashes, it would be a freak\naccident for both the original file and the corrupted file to have the same\nhash!)\n## Putting it All Together\n#### Problem 1: Hash Set Class\n\nTry it yourself →\nImplement a hash set which supports `add()`, `contains()`, `remove()`, and `size()`.\n#### Solution 1: Hash Set Class\n\nFull code & other languages →\nOur final set implementation will consist of a dynamic array of \\*buckets\\*, each bucket being itself a dynamic array of elements.7 The outer array is called a \\*hash table\\*. The number of elements is called the \\*size\\*, and the number of buckets is called the \\*capacity\\*. We can start with a modest capacity, like `10`:\n```\nclass HashSet:\ndef \\_\\_init\\_\\_(self):\nself.capacity = 10\nself.\\_size = 0\nself.buckets = [[] for \\_ in range(self.capacity)]\ndef size(self):\nreturn self.\\_size\n```\nIn this context, size has the same meaning as for dynamic arrays, but capacity is used a bit differently, since all buckets can be used from the beginning. We use a hash function `h()` to determine in which bucket to put elements: element `x` goes in the bucket at index `h(x, m)`, where `m` is the number of buckets in the hash table (the capacity).8 The specific hash function will depend on the type of the elements, but using a good hash function will guarantee that the buckets' lengths are well-balanced.\n```\ndef contains(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn True\nreturn False\n```\nThe final piece to complete our set data structure involves how to dynamically adjust the number of buckets based on the number of elements in the set. On the one hand, we do not want to waste space by having a lot of empty buckets. On the other hand, \\*\\*in order for lookups to take constant time, each bucket should contain at most a constant number of elements.\\*\\* The `size/capacity` ratio is called the \\*load factor\\*. In other words, the load factor is the average length of the buckets.\nA good load factor to aim for is `1`. That means that, on average, each bucket contains one element. Of course, some buckets will be longer than others, depending on how lucky we get with collisions, which are unavoidable. But, if the hash function manages to map elements to buckets approximately uniformly, with a load factor of `1` the \\*\\*expected\\*\\* longest bucket length is still constant.\nRemember how, with dynamic arrays, we had to resize the fixed-size array whenever we reached full capacity? With hash tables, we will do the same: we will relocate all the elements to a hash table twice as big whenever we reach a load factor of `1`.9 The only complication is that when we move the elements to the new hash table, we will have to re-hash them because the output range of the hash function will have also changed. We now have all the pieces for a full set implementation.\n```\ndef add(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn\nself.buckets[hash].append(x)\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\ndef resize(self, new\\_capacity):\nnew\\_buckets = [[] for \\_ in range(new\\_capacity)]\nfor bucket in self.buckets:\nfor elem in bucket:\nhash = h(elem, new\\_capacity)\nnew\\_buckets[hash].append(elem)\nself.buckets = new\\_buckets\nself.capacity = new\\_capacity\n```\nFinally, when removing elements from the set, we downsize the number of buckets if the load factor becomes too small, similar to how we did for dynamic arrays. We can halve the number if the load factor drops below 25%.\n```\ndef remove(self, x):\nhash = h(x, self.capacity)\nfor i, elem in enumerate(self.buckets[hash]):\nif elem == x:\nself.buckets[hash].pop(i)\nself.\\_size -= 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor < 0.25 and self.capacity > 10:\nself.resize(self.capacity // 2)\n```\nThis completes our set implementation.\n![Evolution of a set of integers after a sequence of additions and removals using multiplicative hashing](/blog/set-and-map-implementations/fig3.png)\n\\*\\*Figure 3.\\*\\* Evolution of a set of integers after a sequence of additions and removals. We are using multiplicative hashing as we described above. Integers 24 and 3 both have hash 8, so they get stored sequentially in the same bucket.\n![Resize of the hash table with a load factor of 6/5 > 1, showing how elements need to be rehashed](/blog/set-and-map-implementations/fig4.png)\n\\*\\*Figure 4.\\*\\* Resize of the hash table with a load factor of 6/5 > 1. We need to rehash every element. (This figure shows 5 buckets for illustration purposes, but our implementation starts with 10.)\n## Hash Set Analysis\n\nThe `contains()` method has to compute the hash of the input and then loop through the elements hashed to a particular bucket. Assuming we have a decent hash function and a reasonable load factor, we expect each slot to be empty or contain very few elements. Thus, the \\*\\*expected runtime\\*\\* for a set of integers is `O(1)`. It is worth mentioning that this expectation only applies with a high-quality hash function!\nThe `add()` method has the same amortized analysis as our dynamic array. Most additions take `O(1)` expected time, since we just find the correct bucket and append the value to it. We \\*occasionally\\* need to resize the array, which takes linear time, but, like with dynamic arrays, this happens so infrequently that the \\*\\*amortized runtime\\*\\* per addition over a sequence of additions is still `O(1)`. The analysis for `remove()` is similar.\nThus, we can expect \\*\\*all the set operations to take constant time or amortized constant time\\*\\*. If the elements are strings, we also need to factor in that hashing a string takes time proportional to the size of the string. One simple way to phrase this is that we can expect the operations to take `O(k)` amortized time, where `k` is the maximum length of any string added to the set.\n## Hash Set Problem Set\n\nThe following problem set asks \\*data structure design\\* questions related to hash sets.\n#### Problem 2: Hash Set Class Extensions\n\nTry it yourself →\nAdd the following methods to our `HashSet` class. For each method, provide the time and space analysis.\n1. Implement an `elements()` method that takes no input and returns all the elements in the set in a dynamic array.\n2. Implement a `union(s)` method that takes another set, `s`, as input and returns a new set that includes all the elements in \\*either\\* set. Do not modify either set.\n3. Implement an `intersection(s)` method that takes another set as input, `s`, and returns a new set that includes only the elements contained in \\*both\\* sets. If the sets have no elements in common, return an empty set. Do not modify either set.\n#### Problem 3: Multiset\n\nTry it yourself →\nA \\*multiset\\* is a set that allows multiple copies of the same element. Implement a `Multiset` class with the following methods:\n```\nadd(x): adds a copy of x to the multiset\nremove(x): removes a copy of x from the multiset\ncontains(x): returns whether x is in the multiset (at least one copy)\nsize(): returns the number of elements in the multiset (including copies)\n```\n\\*\\*Problem set solutions.\\*\\*\n#### Solution 2: Hash Set Class Extensions\n\nFull code & other languages →\n1. `elements()`: We can start by initializing an empty dynamic array. Then, we can iterate through the list of buckets, and append to the list all the elements in each bucket. The runtime is `O(capacity + size) = O(size)` (since the load factor is at least 25%, so the capacity is at most four times the size), and the space complexity is `O(size)`.\n2. `union(s)`: We can start by initializing an empty set. Then, we can iterate through the elements in both `self` and `s` and call `add(x)` on the new set for each element `x`.\n3. `intersection(s)`: We can start by initializing an empty set. Then, we can iterate through the elements in `self` and call `add(x)` on the new set only for those elements where `s.contains(x)` is true.\n#### Solution 3: Multiset\n\nFull code & other languages →\nWe can approach it in two ways:\n1. We can tweak the `HashSet` implementation we already discussed. The `contains()`, `remove()`, and `size()` methods do not change. For `add()`, we need to remove the check for whether the element already exists.\n2. Alternatively, we could implement the multiset as a hash map with the elements in the set as keys and their counts as values.\nApproach 2 is more common because it can save space: if we add `n` strings of length `k`, we'll use only `O(k)` space instead of `O(n \\* k)` for Approach 1. See the next section for more on maps.\n# Hash Map Implementation\n\nAs we have seen, sets allow us to store unique elements and provide constant time lookups (technically, it is amortized constant time, and only with a good hash function). Maps are similar, but the elements in a map are called \\*keys\\*, and each key has an associated \\*value\\*. Like elements in sets, map keys cannot have duplicates, but two keys can have the same value. The type of values can range from basic types like booleans, numbers, or strings to more complex types like lists, sets, or maps.\nMap API:\n```\nadd(k, v): if k is not in the map, adds key k to the map with value v\nif k is already in the map, updates its value to v\nremove(k): if k is in the map, removes it from the map\ncontains(k): returns whether the key k is in the map\nget(k): returns the value for key k\nif k is not in the map, returns a null value\nsize(): returns the number of keys in the map\n```\nFor instance, we could have a map with employee IDs as keys and their roles as values. This would allow us to quickly retrieve an employee's role from their ID.\nThankfully, we do not need to invent a new data structure for maps. Instead, we reuse our `HashSet` implementation and only change what we store in the buckets. Instead of just storing a list of elements, we store a list of key–value pairs. As a result, maps achieve \\*\\*amortized constant time for all the operations\\*\\* like sets.\nWe will leave the details of map internals as our final problem set in this chapter. In particular, you can find the full `HashMap` implementation in the solution to Problem 4 (bctci.co/hash-map-class).\n## Hash Map Problem Set\n\nThe following problem set asks \\*data structure design\\* questions related to maps.\n#### Problem 4: Hash Map Class\n\nTry it yourself →\nImplement a `HashMap` class with the API described in the 'Hash Map Implementation' section.\nHint: you can modify the existing `HashSet` class to store key–value pairs.\n#### Problem 5: Hash Map Class Extensions\n\nTry it yourself →\nProvide the time and space analysis of each method.\n1. Implement a `keys()` method that takes no input and returns a list of all keys in the map. The output order doesn't matter.\n2. Implement a `values()` method that takes no input and returns a list of the values of all the keys in the map. The output order doesn't matter. If a value appears more than once, return it as many times as it occurs.\n#### Problem 6: Multimap\n\nTry it yourself →\nA \\*multimap\\* is a map that allows multiple key–value pairs with the same key. Implement a `Multiset` class with the following methods:\n```\nadd(k, v): adds key k with value v to the multimap, even if key k is already found\nremove(k): removes all key-value pairs with k as the key\ncontains(k): returns whether the multimap contains any key-value pair with k as the key\nget(k): returns all values associated to key k in a list\nif there is none, returns an empty list\nsize(): returns the number of key-value pairs in the multiset\n```\n\\*\\*Problem set solutions.\\*\\*\n#### Solution 4: Hash Map Class\n\nFull code & other languages →\nWe can adapt the `HashSet` class. The main difference is that, in `add()`, we need to consider the case where the key already exists, and we just update its value.\nThis was the `add()` method for the `HashSet` class:\n```\ndef add(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn\nself.buckets[hash].append(x)\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\n```\nAnd this is the modified version for the `HashMap` class:\n```\ndef add(self, k, v):\nhash = h(k, self.capacity)\nfor i, (key, \\_) in enumerate(self.buckets[hash]):\nif key == k:\nself.buckets[hash][i] = (k, v)\nreturn\nself.buckets[hash].append((k, v))\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\n```\nThe `get()` method is very similar to `contains()`.\n#### Solution 5: Hash Map Class Extensions\n\nFull code & other languages →\n1. `keys()`: This is analogous to the `elements()` method from the Set Quiz.\n2. `values()`: This is like `keys()`, but getting the values instead.\n#### Solution 6: Multimap\n\nFull code & other languages →\nA multimap with values of type `T` can be implemented as a map where the values are arrays of elements of type `T`. The `add()` method appends to the array, the `get()` method returns the array, and `remove()` works as usual.\n# Rolling Hash Algorithm\n\nBefore wrapping up the chapter, we will illustrate a clever application of hash functions outside of hash sets and maps. Recall 'Problem 3: String Matching' from the String Manipulation chapter (bctci.co/string-matching):\nImplement an `index\\_of(s, t)` method, which returns the first index where string `t` appears in string `s`, or `-1` if `s` does not contain `t`.\nA naive approach would be to compare `t` against every substring of `s` of the same length as `t`. If `sn` and `tn` are the lengths of `s` and `t`, there are `sn - tn + 1` substrings of `s` of length `tn`. Comparing two strings of length `tn` takes `O(tn)` time, so in total, this would take `O((sn - tn + 1) \\* tn) = O(sn \\* tn)` time.\nWe can do better than this naive algorithm. The Knuth-Morris-Pratt (KMP) algorithm is a famous but tricky algorithm for this problem that runs in `O(sn)` time, which is optimal. Here, we will introduce the \\*rolling hash algorithm\\*, another famous algorithm that is also optimal and which we find easier to learn and replicate in interviews.\nThe basic idea is to compare string hashes instead of strings directly. For instance, if `s` is `\"cbaabaa\"` and `t` is `\"abba\"`, we compare `h(\"abba\")` with `h(\"cbaa\")`, `h(\"baab\")`, `h(\"aaba\")`, and `h(\"abaa\")`. If no hashes match, we can \\*conclusively\\* say that `t` does not appear in `s`. If we have a match, it \\*could\\* be a collision, so we need to verify the match with a regular string comparison. However, with a good hash function, 'false positives' like this are unlikely.\nSince we are not actually mapping strings to buckets like in a hash table, we can use the `str\\_to\\_num()` function that we created in the 'Hashing strings' section as our hash function.\nHowever, computing the hash of each substring of `s` of length `tn` would still take `O(sn \\* tn)` time, so this alone doesn't improve upon the naive solution. The key optimization is to compute a \\*rolling hash\\*: we use the hash of each substring of `s` to compute the hash of the next substring efficiently.\n![The hashes of all the substrings of s = 'cbaabaa' of length 4](/blog/set-and-map-implementations/fig5.png)\n\\*\\*Figure 5.\\*\\* The hashes of all the substrings of length 4 of s = 'cbaabaa'.\nSo, how can we go from, e.g., `h(\"cbaa\")` to `h(\"baab\")`? We can do it in three steps. Recall that `ASCII('a') = 97`, `ASCII('b') = 98`, and `ASCII('c') = 99`. We start with:\n`h(\"cbaa\") = 99 \\* 128^3 + 98 \\* 128^2 + 97 \\* 128^1 + 97 \\* 128^0 = 209236193`\nThen:\n1. First, we remove the contribution from the first letter, `'c'`, by subtracting `99 \\* 128^3`. Now, we have:\n`98 \\* 128^2 + 97 \\* 128^1 + 97 \\* 128^0 = 1618145`\n2. Then, we multiply everything by `128`, which effectively increases the exponent of the `128` in each term by one. This is kind of like \\*shifting\\* every letter to the left in Figure 5. Now, we have:\n`98 \\* 128^3 + 97 \\* 128^2 + 97 \\* 128^1 = 207122560`\n3. Finally, we add the contribution of the new letter, `'b'`, which is `98 \\* 128^0`. We end up with:\n`98 \\* 128^3 + 97 \\* 128^2 + 97 \\* 128^1 + 98 \\* 128^0 = 207122658`\nThis is `h(\"baab\")`. We are done!\nHere is a little function to do these steps. Note that we use modulo (`%`) at each step to always keep the values bounded within a reasonable range.10\n```\n# Assumes that current\\_hash is the hash of s[i:i+tn].\n# Assumes that first\\_power is 128^(tn-1) % LARGE\\_PRIME.\n# Returns the hash of the next substring of s of length tn: s[i+1:i+tn+1].\n\ndef next\\_hash(s, tn, i, current\\_hash, first\\_power):\nres = current\\_hash\n# 1. Remove the contribution from the first character (s[i]).\n\nres = (res - ord(s[i]) \\* first\\_power) % LARGE\\_PRIME\n# 2. Increase every exponent.\n\nres = (res \\* 128) % LARGE\\_PRIME\n# 3. Add the contribution from the new character (s[i+tn]).\n\nreturn (res + ord(s[i + tn])) % LARGE\\_PRIME\n```\nEach step takes constant time, so, from the hash of a substring of `s`, we can compute the hash of the \\*\\*next\\*\\* substring in constant time.\nWe pass `128^(tn-1) % 10^9 + 7` as the `first\\_power` parameter so that we do not need to compute it for each call to `next\\_hash()`, as that would increase the runtime. This way, we compute the hashes of all the substrings of length `t` in `O(sn)` time. Putting all the pieces together, we get the rolling hash algorithm:\n```\nLARGE\\_PRIME = 10\\*\\*9 + 7\ndef power(x):\nres = 1\nfor i in range(x):\nres = (res \\* 128) % LARGE\\_PRIME\nreturn res\ndef index\\_of(s, t):\nsn, tn = len(s), len(t)\nif tn > sn:\nreturn -1\nhash\\_t = str\\_to\\_num(t)\ncurrent\\_hash = str\\_to\\_num(s[:tn])\nif hash\\_t == current\\_hash and t == s[:tn]:\nreturn 0 # Found t at the beginning.\n\nfirst\\_power = power(tn - 1)\nfor i in range(sn - tn):\ncurrent\\_hash = next\\_hash(s, tn, i, current\\_hash, first\\_power)\nif hash\\_t == current\\_hash and t == s[i + 1:i + tn + 1]:\nreturn i+1\nreturn -1\n```\nWith a good hash function like the one we used, which makes collisions extremely unlikely, this algorithm takes `O(sn)` time, which is optimal.\nThis algorithm could also be called the \\*sliding\\* hash algorithm since it is an example of the sliding window technique that we learned about in the Sliding Window chapter (bctci.co/sliding-windows).\n# Key Takeaways\n\nThe following table should make it clear why hash sets and hash maps, as we built them in this chapter, are one of the first data structures to consider when we care about performance and big O analysis:\n| Hash Sets | Time | Hash Maps | Time |\n| --- | --- | --- | --- |\n| `add(x)` | O(1) | `add(k, v)` | O(1) |\n| `remove(x)` | O(1) | `remove(k)` | O(1) |\n| `contains(x)` | O(1) | `contains(k)` | O(1) |\n| `size()` | O(1) | `get(k)` | O(1) |\n| | | `size()` | O(1) |\n\\*\\*Table 1.\\*\\* Set and map operations and worst-case runtimes.\nHowever, it is important to remember the caveats:\n\n- The runtimes for `add()` and `remove()` are \\*\\*amortized\\*\\* because of the dynamic resizing technique, which can make a single `add()` or `remove()` slow from time to time.\n\n- All the runtimes (except `size()`) are \\*\\*expected runtimes\\*\\*, and only when we are using a good hash function. In this chapter, we saw examples of good hash functions for integers and strings, and similar ideas can be used for other types.\n\n- A common mistake is forgetting that hashing a string takes time proportional to the length of the string, so hash set and hash map operations do, in fact, \\*\\*not\\*\\* take constant time for non-constant length strings.\nThe 'Sets & Maps' chapter (bctci.co/sets-and-maps) shows how to make the most of these data structures in the interview setting.\n## Footnotes\n\n1. Unlike arrays, sets do not have index-based setters and getters because elements are not in any particular order. ↩\n2. A bad choice for the constant between `0` and `1` would be a simple fraction like `1/2`, because then all even numbers would get hashed to `0` and all odd numbers would get hashed to `m/2`. Here, we are using the inverse of the golden ratio, `φ = 1.618033...` because it is the constant Donald Knuth used when he introduced this technique in his book The Art of Computer Programming. When using `1/φ` as the constant, the hash function is known as \"Knuth's multiplicate hashing,\" as well as \"Fibonacci hashing\" because of the relationship between the golden ratio and the fibonacci sequence. However, the exact constant used is not the point—you do not need to memorize any \"magic constants\" for your interviews. ↩\n3. Mathematicians call this the \"pigeonhole principle\". If you think of the inputs as pigeons and the hashes as holes, it says that if there are more pigeons than holes, some pigeons (inputs) will have to share a hole (have the same hash). ↩\n4. See the 'String Manipulation' Chapter in BCtCI for more on the ASCII format. ↩\n5. Intermediate values may still exceed `32` bits, so, for typed languages, make sure to use 64-bit integers for the intermediate operations. ↩\n6. There is nothing special about the number `10^9 + 7`—it is just a large prime that fits in a 32-bit integer and is easy to remember because it is close to a power of ten. Any large prime would work well. Again, it is not important to memorize magic constants for interviews—you can always use a placeholder variable \"LARGE\\\\_PRIME\". ↩\n7. Another popular data structure for each bucket is a linked list, which we learned about in the Linked Lists chapter (bctci.co/linked-lists). ↩\n8. This strategy of handling collisions by putting all the colliding elements in a list is called \\*separate chaining\\*. There are other strategies, like \\*linear probing\\* or \\*cuckoo hashing\\*, but knowing one is probably enough. ↩\n9. The exact load factor value is not important. Values slightly less or slightly more than one still have the same properties. Between a load factor of `0.5` and `1.5` there is a bit of a trade off between space and lookup time, but the asymptotic complexities don't change. ↩\n10. Depending on the language, we have to be careful about Step 1 because the numerator (`res - ord(s[i]) \\* first\\_power`) can be negative. In Python, `-7 % 5 = 3`, which is what we want, but in other languages like JS, Java, and C++, `-7 % 5 = -2`, which is \\*\\*not\\*\\* what we want. We can fix it by adding `LARGE\\_PRIME` to the result, and applying `% LARGE\\_PRIME` once again. ↩",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/set-and-map-implementations?category=dsa"
    },
    {
      "title": "Top-K Problems: Sorting vs Heaps vs Quickselect",
      "content": "# Top-K Problems: Sorting vs Heaps vs Quickselect\n\nJuly 31, 2025\n![Top-K Problems: Sorting vs Heaps vs Quickselect](/blog/top-k-problems/quickselect1.png)\nOne of the most classic problems in coding interviews is:\n> \\*\\*Top-K Problem:\\*\\* Given an array of `n` integers, and an integer `k ≤ n`, return the `k` smallest values.\nWe'll consider two variants: one in which we need to return the elements in sorted order, and the other in which we can return them in any order.\nIn this post, we'll explain the four main approaches for this problem and show the Python implementations.\n## Overview\n\nThe following table summarizes the different approaches and runtimes:\n| \\*\\*Approach\\*\\* | \\*\\*Unsorted Output\\*\\* | | \\*\\*Sorted Output\\*\\* | |\n| --- | --- | --- | --- | --- |\n| | Time | Space | Time | Space |\n| Sorting | `O(n log n)` | `O(n)` | `O(n log n)` | `O(n)` |\n| Min Heap | `O(n + k log n)` | `O(n)` | `O(n + k log n)` | `O(n)` |\n| Max Heap of size k | `O(n log k)` | `O(k)` | `O(n log k)` | `O(k)` |\n| Quickselect | `O(n)` | `O(n)` | `O(n + k log k)` | `O(n)` |\n| Impossible? | `O(n)` | `O(k)` | `O(n + k log k)` | `O(k)` |\n\\*\\*Table 1.\\*\\* Time and space complexity comparison of different approaches for top-k problems.\nAssumptions:\n\n- Even though the problem is worded in terms of integers, we want to support generic comparison logic for the elements, so the `O(n log n)` sorting lower bound applies. Otherwise, specialized integer sorting algorithms like radix sort could affect the analysis.\n\n- If we modify the input array, we count that as using space.\nWe can see that:\n\n- Quickselect is best to optimize runtime, while heaps are best to optimize space.\n\n- The best possible time depends on whether the output needs to be sorted or not.\n\n- As of now, I don't know of any algorithm that combines the runtime of Quickselect and the space complexity of the max-heap solution (the \"Impossible?\" row).\nInstead of the `k` smallest values, we could also ask for the `k` largest values. The problem is essentially the same.\nThis blog post is based on Beyond Cracking the Coding Interview content.\nCredit in particular to Mike Mroczka, who did\na lot of the writing in the Sorting and Heaps chapters. You can find all four\napproaches in \\*\\*Python, JS, Java, and C++\\*\\* in the online materials -- see the\nFirst K problem. By clicking the link, you can\neven try the problem yourself with our AI interviewer! The online materials\nare free, but behind a login wall.\n## Sorting solution\n\nThis question is trivial if we sort the input. After sorting, we get the first `k` elements.\n```\n# O(n log n) time, O(n) space\n\ndef sorting\\_approach(arr, k):\narr.sort()\nreturn arr[:k] # Sorted output\n\n```\nEven if we sort the array in-place, keep in mind that most built-in sorting libraries use `O(n)` \\*\\*extra\\*\\* space internally.\n## Min-heap solution\n\nSorting is 'overkill' because, even if we need to sort the elements we return, we don't need to sort the rest.\nA natural approach for problems involving \\*partial sorts\\* is to use a heap. We could dump the array into a \\*\\*min-heap\\*\\* and pop the `k` smallest numbers:\n```\n# O(n + k log n) time, O(n) space\n\ndef min\\_heap\\_approach(arr, k):\nheapq.heapify(arr)\nreturn [heapq.heappop(arr) for \\_ in range(k)] # Sorted output\n\n```\nThis approach leverages that we can build a heap in linear time with heapify. After that, we do `k` heap pops. Since the heap has `O(n)` elements, each pop takes `O(log n)` time.\nTo learn more about heaps, check out the Implement a Heap problem in the BCtCI online materials.\n## Max-heap solution\n\nAlternatively, we could use \\*\\*max-heap\\*\\* and keep track of the smallest `k` elements as we iterate through them. We use a max-heap so that the largest element among the smallest `k` is always at the root, making it easy to remove it when we need to make space for a smaller element.\nThe algorithm is simple: for each element, we push it to the max-heap. If the heap size exceeds `k`, we pop the largest element (which is guaranteed to be at the root of the max-heap). This ensures that the heap always contains the `k` smallest elements seen so far.\n```\n# O(n log k) time, O(k) space\n\ndef max\\_heap\\_approach(arr, k):\nmax\\_heap = []\nfor num in arr:\nheapq.heappush(max\\_heap, -num) # Negate values to simulate a max-heap\n\nif len(max\\_heap) > k:\nheapq.heappop(max\\_heap)\nreturn [-x for x in max\\_heap] # Unsorted output\n\n```\nSince the heap size never exceeds `k`, each heap operation only takes `O(log k)` time. However, we don't benefit from heapify.\nIf we wanted a sorted output, we can pop the elements from the max-heap one by one instead of directly returning the underlying array of heap elements. This would add an extra `O(k log k)` factor to the runtime, but the `O(n log k)` still dominates.\nThis is the most space-efficient solution.\n## Quickselect solution\n\nTo get to the most efficient solution, we need to realize that we can use this property:\n\\*\\*Property 1:\\*\\* We only need to find the `k`th smallest element. After that, we can do a single pass to get all the elements equal to or smaller than it.\nSo, how do we find the `k`th smallest element? This is the exact problem solved by the \\*Quickselect\\* algorithm.\nFor now, assume that we had a `quickselect()` function that returns the `k`th smallest element of an array.\nHere is how we would use it:\n```\n# O(n) time, O(n) space\n\ndef quickselect\\_approach(arr, k):\nkth\\_smallest = quickselect(arr, k)\nresult = [x for x in arr if x < kth\\_smallest]\nwhile len(result) < k:\nresult.append(kth\\_smallest)\nreturn result # Unsorted output -- return sorted(result) for sorted output.\n\n```\nThe `while` loop is necessary because of duplicates in the array. If we initialized `res` as the subset of `arr` that is \\*\\*less than or equal\\*\\* to `kth\\_smallest`, we may have ended with more than `k` elements if `arr` contains duplicates of `kth\\_smallest`. It's easier to first add all the elements less than `kth\\_smallest`, and then add as many copies of `kth\\_smallest` as needed to reach `k` elements.\nAs we'll see, `quickselect()` runs in `O(n)` time. If we don't need to sort the output, Quickselect is the bottleneck.\nIf we need to output the `k` smallest elements sorted, the runtime becomes `O(n + k log k)`.\nFinally, let's show how to implement the `quickselect()` function.\n### Quickselect\n\nRecall the Quicksort partition step:\n```\n# O(n) time, O(n) space\n\ndef partition(arr):\npivot = random.choice(arr)\nsmaller, equal, larger = [], [], []\nfor x in arr:\nif x < pivot:\nsmaller.append(x)\nelif x == pivot:\nequal.append(x)\nelse:\nlarger.append(x)\nreturn smaller, equal, larger\n```\nIf the array can contain duplicates, `equal` may contain multiple elements:\n![Quickselect Partition](/blog/top-k-problems/quickselect1.png)\nThe key is to think about the position of the pivot:\n\\*\\*Property 2:\\*\\* After the quicksort partition step, we know exactly how many elements are smaller than the pivot. Thus, we can deduce the actual index of the pivot (or indices, if the pivot has duplicates) in the \\*\\*sorted\\*\\* order.\nLet `S` and `E` represent the number of elements in the `smaller` and `equal` arrays, respectively. We can do a \\*\\*case analysis\\*\\* to see whether the `k`th smallest element is in `smaller`, `equal`, or `larger`:\n1. Case 1: `k <= S`. The `k`th smallest element is in `smaller`.\n2. Case 2: `S < k <= S + E`. The `k`th smallest element is in `equal`.\n3. Case 3: `k > S + E`. The `k`th smallest element is in `larger`.\nIn Quickselect, we do this case analysis and act accordingly:\n1. Case 1: We can recursively search for the `k`th smallest element in `smaller`.\n2. Case 2: The `pivot` \\*is\\* the `k`th smallest element. We return it directly.\n3. Case 3: We can recursively search for the `(k - S - E)`th smallest element in `larger`.\nFor example, if `S = 60`, `E = 10`, and `L = 30`, and `k = 76`, we would need to search for the 6th smallest element in `larger`.\nThe implementation is similar to quicksort. In particular, the partition logic is the same. The main difference is that we have a single recursion call depending on the case analysis, a bit like in binary search.\n```\n# O(n) time, O(n) space\n\ndef quickselect(arr, k):\nsmaller, equal, larger = partition(arr)\nS, E = len(smaller), len(equal)\nif k <= S:\nreturn quickselect(smaller, k)\nelif k <= S + E:\nreturn equal[0]\nelse:\nreturn quickselect(larger, k - S - E)\n```\nHere is a full example, where we are looking for the 5th smallest element. The 5th smallest element is highlighted in red.\n![Quickselect example](/blog/top-k-problems/quickselect2.png)\nQuickselect is not common in interviews, but it's not hard to implement if you already understand quicksort, so you can add it to your bag of tricks.\n### Quickselect analysis\n\nQuickselect runs in linear time with high probability, but the analysis is a bit tricky because it depends on how lucky we get with the pivots.\nTo be clear, the analysis below is beyond what's expected for coding interviews.\nThe partition step takes `O(n)` time.\nThe key question is: when we make a recursive call, how small is the new array (`smaller` or `larger`) relative to `arr`? How much is the array size reduced at each step?\nIf we are consistently unlucky, we might reduce the array size by only one at each recursive call. For instance, if `k` is `1`, and the random pivot happens to be the largest element, we'll only eliminate `1` element (the pivot itself). That's why, in the worst case with the worst possible luck, the number of steps is `O(n)`, and the total runtime is `O(n^2)`. However, the probability of this happening is negligible (approaches `0` exponentially fast as `n` grows).\nAs you probably know from the analysis of binary search, if the array is reduced by 50% at each step, the number of steps is `log\\_2(n)`. (The definition of `log\\_2(n)` is \"the number of times you can divide `n` by `2` until you get `1`\".)\nHowever, we don't need the reduction to be 50% at each step. As long as the reduction is at least some constant fraction at each step (like, e.g., 25%), the number of steps is still logarithmic, just with a different base. In the end, it's still `O(log n)`, since the base of logarithms goes away in big O notation.\nThe challenge with Quickselect is that we don't always reduce the array by a constant fraction.\nThe key is that, since the pivot is random, there's a 50% chance that the pivot will be in the middle `n/2` elements of `arr`.\nIf that happens, \\*at least\\* 25% of the elements will be eliminated from the search range:\n\n- If `k` is larger than the pivot's position, we'll eliminate the `n/4` smallest elements.\n\n- If `k` is smaller than the pivot's position, we'll eliminate the `n/4` largest elements.\nEliminating a constant fraction of the elements (at least 25%) with a 50% chance is enough to guarantee that the number of recursive calls is `O(log n)` with high probability. As `n` grows, the probability of being unlucky becomes negligible.\nThe final recurrence for the expected worst-case runtime of Quickselect is:\n`T(n) = O(n) \\* s + T(0.75n)`\nWhere:\n\n- `O(n)` is the runtime of the partition step.\n\n- `s` is the number of steps until the array gets reduced by at least 25%.\n\n- `T(0.75n)` is the runtime of the remaining 75% of the array recursively.\nAs we discussed, there's a >50% chance that `s` is `1`, so the expected value of `s` is `O(1)`. This leaves:\n`T(n) = O(n) + T(0.75n) = O(n + 0.75n + (0.75)^2 n + ...)`\nThe infinite geometric series `1 + 0.75 + (0.75)^2 + ...` converges to `4`, so we can simplify the whole thing to `T(n) = O(4n) = O(n)`.\n### Quickselect variations\n\nIf we don't want to deal with expected runtimes, there's a method to pick good pivots deterministically, called Median of Medians. It guarantees a deterministic worst-case runtime of `O(n)`. However, the random pivot is faster in practice and easier to implement (especially relevant in interviews).\nThere's also an in-place version of Quickselect. The version we showed takes `O(n)` expected extra space because we declare new `smaller`, `equal`, and `larger` arrays.\nIf we are allowed to modify the input, there is an in-place version of Quickselect that rearranges the array in-place to put the smaller elements first, then the pivot(s), and then the larger elements. Then, the expected extra space would be `O(log n)`, with the recursion stack as the bottleneck. With an iterative implementation, we can bring this further down to `O(1)`.\n## Conclusion\n\nThis post shows one of the connections between sorting, heaps, and Quickselect. In interviews, whenever a problem involves sorting in some capacity, consider if one of these adjacent approaches can yield a better solution.\nTo learn more about these topics and more, at a level specifically targeting\ncoding interviews, check out Beyond Cracking the Coding Interview on\nAmazon.\nI'll end with a question:\n\\*\\*Is it possible to achieve the runtime of Quickselect with the space complexity of the max-heap solution?\\*\\*\nThat is:\n\n- Unsorted output variant: can it be solved in `O(n)` time and `O(k)` extra space without modifying the input?\n\n- Sorted output variant: can it be solved in `O(n + k log k)` time and `O(k)` extra space without modifying the input?\nThe answer is probably already known, but I haven't looked into it.\nIf you have any ideas, please let me know!\n\n---\n\\*Want to leave a comment? You can post under the linkedin post or the X post.\\*",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/top-k-problems?category=dsa"
    },
    {
      "title": "Queues in JS interviews",
      "content": "JS doesn't have built-in queues, which can be an issue if you have to implement a BFS. Here are the workarounds.",
      "content_type": "podcast_transcript",
      "source_url": "https://nilmamano.com/blog/js-queues?category=dsa"
    },
    {
      "title": "In Defense of Coding Interviews",
      "content": "A collection of arguments in favor of coding interviews.",
      "content_type": "podcast_transcript",
      "source_url": "https://nilmamano.com/blog/in-defense-of-coding-interviews?category=dsa"
    },
    {
      "title": "Get Binary Search Right Every Time, Explained Without Code",
      "content": "# Get Binary Search Right Every Time, Explained Without Code\n\nApril 15, 2025\n![Get Binary Search Right Every Time, Explained Without Code](/blog/binary-search/cover.png)\nOne of the things that makes binary search tricky to implement is that you\nusually need to tweak the pointer manipulation logic in subtle ways based on the\nspecifics of the problem.\nE.g., an implementation that works for finding a target in a sorted array when the target is present, may not work if the target is missing. Or, it may not be clear how to tweak the code to find the last occurrence of the target instead of the first one. And of course, there are plenty of less conventional applications of binary search where the input is not an array, like catching bike thieves.\nIn Beyond Cracking the Coding Interview, we wanted to simplify this, so we went looking for a general binary search template. Going into it, I thought we might need at least two templates, but we ended up with just one, which we called the \"transition point recipe\", and which works for every problem we tried, including the 17 problems in the binary search chapter of the book. If you find one where it doesn't work, let me know!\n## The transition point problem\n\nHere is the thesis of the transition point recipe:\nEvery binary search problem can be reduced to the 'transition point problem'.\nIn the 'transition point problem', you are given an array with just two values, say `1` and `2`, where all the `1`s come before the `2`s, and you need to point where it changes.\nE.g., in the array `[1, 1, 1, 1, 1, 2, 2, 2]`, the last `1` is at index `4` and the first `2` is at index `5`.\nKnowing how to solve this specific problem is key to our recipe. The specific binary search implementation is not important, but there is an invariant we can follow that makes it quite easy: ensure that the left pointer is always at a `1` and the right pointer is always at a `2`.\nWe give code in the book, but remembering exact code in an interview is error prone. Instead, the four bullet points below are all I \\*personally\\* remember, and I feel confident that I can derive the rest easily.\n1. Start by handling some edge cases:\n\n- The array is empty\n\n- Every value is `1`\n\n- Every value is `2`\n2. Initialize two pointers, `left` and `right`, to the first and last indices, respectively.\n3. For the main binary search loop, always maintain the \\*invariant\\* that the value at `left` is `1` and the value at `right` is `2`. Let this invariant guide your pointer manipulation logic, so that you don't need to memorize any code.\n4. Stop when the `left` and `right` pointers are next to each other (i.e., `left + 1 == right`).\nCombining the invariant with the stopping condition, we get that, at the end, `left` will be at the last `1` and `right` will be at the first `2`.\nThese bullet points rely on two ideas to make binary search easier: (1) handling edge cases upfront, and (2) letting strong invariants guide the implementation. Notice how the invariant even guides the edge cases at the beginning, as they are the necessary ones to be able to initialize `left` and `right` in a way that satisfies it.\n## The reduction\n\nOk, so now, let's take for granted that we can solve the transition point problem. How does this help us solve other binary search problems?\nThe idea is to come up with a (problem-specific) \\*predicate\\*, like `< target`, `>= target`, or `x % 2 == 0`, which splits the search range into two regions, the \"before\" region and the \"after\" region.\nThis predicate is a function that takes an element of the search range and returns a boolean, and -- as you probably saw coming -- it is key that all the elements with `true` values come before the elements with `false` values (or the other way around).\nThen, we can use the solution to the transition point problem to find the transition point between the 'before' and 'after' regions. The only difference is that, instead of checking boolean values directly, we check the result of the predicate.\nYou can even wrap the predicate in a function, which we called `is\\_before(x)` in the book, which tells you whether a given element is in the 'before' region. Then, it's really obvious that we are just solving the transition point problem every time.\nThe only part that requires some thinking is choosing the right transition point. For example:\n\n- if we want to find the \\*first\\* occurrence of `target` in a sorted array, we can use `is\\_before(x) = x < target`, which means that, if `target` is present, the first occurrence is the first element in the 'after' region (so, we can check/return the `right` pointer at the end).\n\n- if we want to find the \\*last\\* occurrence of `target` in a sorted array, we can use `is\\_before(x) = x <= target`, which means that, if `target` is present, the last occurrence is the last element in the 'before' region (so, we can check/return the `left` pointer at the end).\nAnd so on for other problems.\n![Binary search recipe](/blog/binary-search/meme.png)\n## Practice problems\n\nYou can try the transition-point recipe on all the problems from the binary search chapter of the book online at start.interviewing.io/beyond-ctci/part-vii-catalog/binary-search, even if you don't have the book. There, you can also find all our solutions using the recipe, in Python, JS, Java, and C++.\nBy the way, the binary search chapter of the book is free -- it's in bctci.co/free-chapters.\n\\*Want to leave a comment? You can post under the linkedin post or the X post.\\*",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/binary-search?category=dsa"
    },
    {
      "title": "Problem Solving BCtCI Style",
      "content": "A problem walkthrough using the concepts from Beyond Cracking the Coding Interview.",
      "content_type": "podcast_transcript",
      "source_url": "https://nilmamano.com/blog/problem-solving-bctci-style?category=dsa"
    },
    {
      "title": "Heapify Analysis Without Math",
      "content": "# Heapify Analysis Without Math\n\nOctober 30, 2024\n![Heapify Analysis Without Math](/blog/heapify-analysis/cover.png)\nI'm writing about heaps for Beyond Cracking the Coding Interview (beyondctci.com), and the most technical part is the analysis of \\*heapify\\*. It's easy to show that it takes `O(n log n)` time, where `n` is the number of nodes in the heap, but it's not straightforward to show that this is not tight and the method actually takes `O(n)`. time.\nEvery proof I have found online involves a summation over the levels of the heap that ends up looking something like the one in Wikipedia heap page:\n![wikipedia formula](/blog/heapify-analysis/wikipedia.png)\nwhich is more math than I want to put in this book (the bulk of the audience consists of people trying to land a SWE job, not math enthusiasts).\nBelow is the proof \"without complicated math\" I came up with that heapify takes `O(n)` time. If you are familiar with the classic proof, let me know if you find it easier - I might use it for the book. Also, please let me know if you've seen someone else proving it in a similar way.\nIf you already know what heapify is, you can jump directly to the Proof.\n## Heap Recap\n\nHeaps are binary trees with two special properties:\n1. They are \\*\\*complete\\*\\* binary trees: all the levels except the last one have the maximum number of nodes; the last level may not be full, but all the nodes are aligned to the left. (In particular, this implies that heaps have logarithmic height, which is key to the big O analysis.)\n2. The \\*\\*heap property:\\*\\* every node is smaller than its children (this is assuming a min-heap - it would be the opposite for a max-heap).\n![Min-heap vs max-heap](/blog/heapify-analysis/heaps.png)\nI will focus on the heapify operation and its analysis, but if you want to learn heaps from scratch, the Algorithms with Attitude Youtube channel has a great video on it. He also covered the classic linear-time proof for heapify, if you want to compare it to mine.\nIn any case, I left a full Python heap implementation at the bottom of this post.\n## What's Heapify?\n\nHeapify (invented by Robert W. Floyd) converts a binary tree which is already complete, but may not have the heap property, into a proper heap.\nHeapify uses the \"bubble-down\" procedure, which starts at a node that may not satisfy the heap property, and recursively swaps it with the smallest of its two children until the heap property is restored:\n![Bubble-down procedure](/blog/heapify-analysis/bubbledown.png)\n```\ndef bubble\\_down(self, idx):\nleft\\_idx, right\\_idx = left\\_child(idx), right\\_child(idx)\nis\\_leaf = left\\_idx >= len(self.heap)\nif is\\_leaf: return # Leaves cannot be bubbled down.\n# Find the index of the smallest child\n\nchild\\_idx = left\\_idx\nif right\\_idx < len(self.heap) and self.heap[right\\_idx] < self.heap[left\\_idx]:\nchild\\_idx = right\\_idx\nif self.heap[child\\_idx] < self.heap[idx]:\nself.heap[idx], self.heap[child\\_idx] = self.heap[child\\_idx], self.heap[idx]\nself.bubble\\_down(child\\_idx)\n```\nHeapify works by \"bubbling down\" every non-leaf (internal) node, from bottom to top:\n![Heapify steps](/blog/heapify-analysis/steps.png)\nThis figure shows the heapify steps for a min-heap. The first tree is the initial state, which doesn't yet have the min-heap property. Leaves are already at the bottom, so bubbling them down has no effect. The next 3 trees show the evolution after bubbling down the two nodes at depth 1 and then the node at depth 0.\nIn the array-based heap implementation, `heapify()` looks like this:\n```\ndef heapify(self, arr):\nself.heap = arr\nfor idx in range(len(self.heap) // 2, -1, -1):\nself.bubble\\_down(idx)\n```\nThe reason why we start bubbling down from the middle of the heap is that, in a complete tree, at least half the nodes are leaves, and we don't need to bubble those down.\nHere, we won't prove that it \\*works\\*, only that its analysis is `O(n)`.\n## Proof\n\nI'll start with a definition and a fact we'll use later:\nA \\*perfect\\* binary tree is a complete tree where the last level is full:\n![Perfect binary tree](/blog/heapify-analysis/perfect.png)\n\\*\\*Fact 1: In a perfect tree, the number of leaves is 1 more than the number of internal nodes.\\*\\*\nFor instance:\n```\n\n- Height 1: 1 leaf, 0 internal nodes, 1 total\n\n- Height 2: 2 leaves, 1 internal node, 3 total\n\n- Height 3: 4 leaves, 3 internal nodes, 7 total\n\n- Height 4: 8 leaves, 7 internal nodes, 15 total\n```\nFact 1 is true because the number of nodes at each level is a power of 2, so:\n\n- the number of leaves is a power of 2, and\n\n- the number of internal nodes is the sum of all the previous powers of 2.\nThe sum of the first few powers of 2 add up to one less than the next power of 2. You can see that if you line them up like this:\n![Sum of powers of 2](/blog/heapify-analysis/zeno.png)\nIt's a bit like Zeno's paradox, where each power of 2 in the sum halves the remaining distance, but never quite gets to 64.\nWith that out of the way, back to heapify:\nIn the worst case, each node will get bubbled down all the way to a leaf. Thus, each node needs to move down `O(log n)` levels, so one might reasonably expect heapify to take `O(n log n)` time. This is correct in the 'upper bound' sense, but not tight: the total time is actually `O(n)`. The intuition for why that is the case is that most nodes are in the deeper levels of the tree, where they don't need to travel a lot to get to the bottom.\nWe'll actually prove a \\*\\*stronger\\*\\* claim:\n\\*\\*Main Claim: If you heapify a perfect tree, the number of 'bubble-down' swaps is smaller than `n`, the number of nodes.\\*\\*\n\n- We'll assume the worst case, in which every node is bubbled down to a leaf position.\n\n- If the claim is true and heapify does `= len(self.heap)\nif is\\_leaf: return # Leaves cannot be bubbled down.\n# Find the index of the smallest child\n\nchild\\_idx = left\\_idx\nif right\\_idx < len(self.heap) and self.heap[right\\_idx] < self.heap[left\\_idx]:\nchild\\_idx = right\\_idx\nif self.heap[child\\_idx] < self.heap[idx]:\nself.heap[idx], self.heap[child\\_idx] = self.heap[child\\_idx], self.heap[idx]\nself.bubble\\_down(child\\_idx)\ndef heapify(self, arr):\nself.heap = arr\nfor idx in range(len(self.heap) // 2, -1, -1):\nself.bubble\\_down(idx)\n```",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/heapify-analysis?category=dsa"
    },
    {
      "title": "Lazy vs Eager Algorithms",
      "content": "# Lazy vs Eager Algorithms\n\nOctober 16, 2024\n![Lazy vs Eager Algorithms](/blog/lazy-vs-eager/cover.png)\nWarning\nI have not tested any code snippet below -- I just wanted to illustrate the\nconcepts. Please let me know if you find a bug.\n## Introduction\n\nMost algorithms have multiple valid implementations. For instance, in a binary tree problem, you have multiple ways of handling NULL nodes. I'm currently writing \\*\\*Beyond Cracking the Coding Interview\\*\\* (beyondctci.com), which means that my co-authors and I need to take a stance on what version of each algorithm to use. Ideally, we want to show the simplest version of each algorithm:\n\n- Easy to recall for interview,\n\n- Easy to explain to interviewers,\n\n- Easy to debug by hand,\n\n- Short, so that it is quick to code.\nIn the book, we don't claim that the version we show is \"the best\" - we say to use the one that works best for you. But showing one in the book is an implicit endorsement.\nOne particular decision that comes up again and again with recursive algorithms is choosing between the \\*\\*lazy\\*\\* version and the \\*\\*eager\\*\\* version of an algorithm.\n\n- An \\*\\*eager\\*\\* recursive function expects 'valid' inputs and ensures to only call the recursive function with 'valid' inputs. We can also call it a \\*\\*clean\\*\\* (call) \\*\\*stack\\*\\* algorithm.\n\n- A \\*\\*lazy\\*\\* recursive algorithm allows 'invalid' inputs, so it starts by validating the input. Then, it calls the recursive function without validating the inputs passed to it. We can also call it a \\*\\*dirty stack\\*\\* algorithm.\nWhat 'valid' means depends on the algorithm--we'll see plenty of examples. We'll also translate the concept of eager vs lazy to iterative algorithms.\n## Lazy vs Eager Tree Traversals\n\nAn \\*\\*eager\\*\\* tree traversal eagerly validates that the children are not NULL before passing them to the recursive function. A \\*\\*lazy\\*\\* tree traversal doesn't, so it needs to check if the current node is NULL before accessing it.\nFor instance, here is eager vs lazy preorder traversal:\n```\nclass Node:\ndef \\_\\_init\\_\\_(self, val, left=None, right=None):\nself.val = val\nself.left = left\nself.right = right\ndef preorder\\_traversal\\_eager(root):\nres = []\n# CANNOT be called with node == None\n\ndef visit(node):\nres.append(node.val)\nif node.left:\nvisit(node.left)\nif node.right:\nvisit(node.right)\nif not root:\nreturn []\nvisit(root)\nreturn res\ndef preorder\\_traversal\\_lazy(root):\nres = []\n# CAN be called with node == None\n\ndef visit(node):\nif not node:\nreturn\nres.append(node.val)\nvisit(node.left)\nvisit(node.right)\nvisit(root)\nreturn res\n```\nBoth have the same runtime and space analysis. Even the constant factors probably don't change much, so it comes down to style preference. Which one do you prefer?\n## Lazy vs Eager graph DFS\n\nAn \\*\\*eager\\*\\* graph DFS eagerly checks that the neighbors are not already visited before passing them to the recursive function. A \\*\\*lazy\\*\\* graph DFS doesn't, so it needs to check if the current node is already visited.\n```\n# Returns all nodes reachable from start\n\ndef dfs\\_eager(adj\\_lists, start):\nres = []\nvisited = set()\ndef visit(node):\nres.append(node)\nfor neighbor in adj\\_lists[node]:\nif neighbor not in visited:\nvisited.add(neighbor)\nvisit(neighbor)\nvisited.add(start)\nvisit(start)\nreturn res\ndef dfs\\_lazy(adj\\_lists, start):\nres = []\nvisited = set()\ndef visit(node):\nif node in visited:\nreturn\nvisited.add(node)\nres.append(node)\nfor neighbor in adj\\_lists[node]:\nvisit(neighbor)\nvisit(start)\nreturn res\n```\nFor a graph DFS, we can also do a mix between lazy and eager: we can eagerly check if nodes are already visited, and lazily mark them as visited:\n```\ndef dfs\\_lazy(adj\\_lists, start):\nres = []\nvisited = set()\ndef visit(node):\nvisited.add(node)\nres.append(node)\nfor neighbor in adj\\_lists[node]:\nif neighbor not in visited:\nvisit(neighbor)\nvisit(start)\nreturn res\n```\nAgain, they all have the same analysis. Which one do you prefer?\n## Lazy vs Eager grid algorithms\n\nConsider the same DFS algorithm but on a grid of 0's and 1's. The 0's are walkable cells, the 1's are obstacles, and\nwalkable cells next to each other are connected. This time, we need to check that the neighbors are not out of bounds, which we can do lazily or greedily.\n```\n# Returns all cells reachable from (start\\_row, start\\_col).\n\ndef grid\\_dfs\\_eager(grid, start\\_row, start\\_col):\nnr, nc = len(grid), len(grid[0])\nres = []\nvisited = set()\ndef visit(row, col):\nres.append((row, col))\nfor dir in ((-1, 0), (1, 0), (0, 1), (0, -1)):\nr, c = row + dir[0], col + dir[1]\nif 0 <= r < nr and 0 <= c < nc and grid[r][c] == 0 and (r, c) not in visited:\nvisited.add((r, c))\nvisit(r, c)\n# Assumes (start\\_row, start\\_col) is within bounds\n\nvisited.add((start\\_row, start\\_col))\nvisit(start\\_row, start\\_col)\nreturn res\ndef grid\\_dfs\\_lazy(grid, start\\_row, start\\_col):\nnr, nc = len(grid), len(grid[0])\nres = []\nvisited = set()\ndef visit(row, col):\nif row < 0 or row >= nr or col < 0 or col >= nc or grid[row][col] == 1:\nreturn\nif (row, col) in visited:\nreturn\nvisited.add((row, col))\nres.append((row, col))\nfor dir in ((-1, 0), (1, 0), (0, 1), (0, -1)):\nvisit(row + dir[0], col + dir[1])\nvisit(start\\_row, start\\_col)\nreturn res\n```\n## Lazy vs Eager Memoization DP\n\nIn a \\*\\*lazy\\*\\* memoization DP (Dynamic Programming) algorithm, we call the recursive function for a subproblem without checking first if we have already computed that subproblem. In an \\*\\*eager\\*\\* algorithm, we only call the recursive function for subproblems that we still need to compute.\n```\n# Returns all cells reachable from (start\\_row, start\\_col).\n\ndef fibonacci\\_eager(n):\nmemo = {}\ndef fib\\_rec(i):\nif i <= 1:\nreturn 1\nif i-1 in memo:\nprev = memo[i-1]\nelse:\nprevprev = fib\\_rec(i-1)\nif i-2 in memo:\nprevprev = memo[i-2]\nelse:\nprev = fib\\_rec(i-2)\nmemo[i] = prev + prevprev\nreturn memo[i]\nreturn fib\\_rec(n)\ndef fibonacci\\_lazy(n):\nmemo = {}\ndef fib\\_rec(i):\nif i <= 1:\nreturn 1\nif i in memo:\nreturn memo[i]\nmemo[i] = fib\\_rec(i-1) + fib\\_rec(i-2)\nreturn memo[i]\nreturn fib\\_rec(n)\n```\nFor memoization DP, I think \\*\\*lazy\\*\\* is cleaner and more conventional.\n## Lazy vs Eager Iterative Tree traversals\n\nConsider a level-order traversal on a binary tree. A level-order traversal is an iterative algorithm that uses a queue data structure.\n\n- A \\*\\*lazy\\*\\* version puts children in the queue without checking if they are NULL first. We can call it a \\*\\*dirty queue\\*\\* algorithm.\n\n- An \\*\\*eager\\*\\* version checks for NULL nodes and avoids putting them in the queue. We can call it a \\*\\*clean queue\\*\\* algorithm.\n```\ndef level\\_order\\_traversal\\_eager(root):\nif not root:\nreturn []\nres = []\nQ = deque([root])\nwhile Q:\nnode = Q.popleft()\nres.append(node.val)\nif node.left:\nQ.append(node.left)\nif node.right:\nQ.append(node.right)\nreturn res\ndef level\\_order\\_traversal\\_lazy(root):\nres = []\nQ = deque([root])\nwhile Q:\nnode = Q.popleft()\nif not node:\ncontinue\nres.append(node.val)\nQ.append(node.left)\nQ.append(node.right)\nreturn res\n```\n## Eager Graph BFS is better than lazy Graph BFS\n\nThis is the first exception where one is better than the other in terms of big O analysis. The \\*\\*lazy\\*\\* BFS allows adding already-visited nodes to the queue, while the \\*\\*eager\\*\\* one does not. We'll first look at the two versions, and then analyze them.\n```\ndef graph\\_bfs\\_eager(adj\\_lists, start):\nres = []\nvisited = set()\nvisited.add(start)\nQ = deque([start])\nwhile Q:\nnode = Q.popleft()\nres.append(node.val)\nfor neighbor in adj\\_lists[node]:\nif neighbor not in visited:\nvisited.add(neighbor)\nQ.append(neighbor)\nreturn res\ndef graph\\_bfs\\_lazy(adj\\_lists, start):\nres = []\nvisited = set()\nQ = deque([start])\nwhile Q:\nnode = Q.popleft()\nif node in visited:\ncontinue\nvisited.add(node)\nres.append(node)\nfor neighbor in adj\\_lists[node]:\nQ.append(neighbor)\nreturn res\n```\nIt may come as a surprise that these two are \\*\\*not\\*\\* equivalent like all the other examples.\nLet's say `V` is the number of nodes and `E` is the number of edges. To keep things simple, consider that the graph is connected, meaning that `E` is at least `V-1` and at most `O(V²)`.\nBoth versions take `O(E)` time. The difference is in the space complexity: the eager version takes `O(V)` space because we never have the same node twice in the queue. The lazy version takes `O(E)` space because we allow the same nodes multiple times in the queue.\nTo see this, consider a complete graph:\n![Complete graph](/blog/lazy-vs-eager/completegraph.png)\n1. When we visit start, we add A, B, C, D, E to the queue. Now the queue is: `[A, B, C, D, E]`\n2. When we visit A, we add start, B, C, D, E to the queue. Now the queue is: `[B, C, D, E, start, B, C, D, E]`\n3. When we visit B, we add start, A, C, D, E to the queue. Now the queue is: `[C, D, E, start, B, C, D, E, start, A, C, D, E]`\n4. And so on.\nBy the time we finish popping the nodes added as neighbors of the start node, we've done `V` queue pops and `V²` queue appends, so the queue size is `O(V²)`.\nSo, why didn't this happen for other lazy algorithms we have seen?\n\n- For tree traversals, each tree node has a single parent that it can be reached from, so we don't need to worry about the same node appearing twice in the call stack or in the level-order traversal queue.\n\n- For graph DFS, \\*\\*every node in the call stack\\*\\* is marked visited, so if we call `visit()` on a node that is already in the call stack, we'll immediately return as we'll see it is marked as visited.\n## Eager Dijkstra is better than Lazy Dijkstra, but harder to implement\n\nI wrote extensively about different Dijkstra implementations in this Dijkstra blog post.\nDijkstra is similar to BFS, with the main difference that it uses a priority queue (PQ) instead of a queue to visit the nodes that are closer first (in terms of shortest paths).\nIn BFS, when a node is added to the queue, its distance from the starting node is already established and there is never a reason to add it again to the queue. In Dijkstra, when a node is added to the PQ, we might later find a shorter path while it is still in the PQ. When that happens, we can do two things:\n\n- \\*\\*Lazy Dijkstra\\*\\*: just add the node again with the new, improved distance. It will get popped before the previous occurrence because it has higher priority in the PQ. When a node with a \"stale\" distance gets popped off from the queue, we just ignore it.\n\n- \\*\\*Eager Dijkstra\\*\\* (called textbook Dijkstra in the other blog post): instead of adding the node again, find the existing occurrence of it in the PQ, and update it with the new found distance. This guarantees that the same node never appears twice in the PQ.\nBoth versions take `O(E\\*log V)` time, but eager is more space efficient, analogously to eager BFS: `O(V)` for eager Dijkstra vs `O(E)` for lazy Dijkstra.\nHere is lazy Dijkstra:\n```\ndef dijkstra\\_lazy(adj\\_lists, start):\ndist = defaultdict(int)\ndist[start] = 0\nvisited = set()\nPQ = [(0, start)]\nwhile PQ:\n\\_, node = heappop(PQ) # Only need the node, not the distance.\n\nif node in visited:\ncontinue # Not the first extraction.\n\nvisited.add(node)\nfor neighbor, weight in adj\\_lists[node]:\nif dist[node]+weight < dist[neighbor]:\ndist[neighbor] = dist[node]+weight\n# Neighbor may already be in the PQ; we add it anyway.\n\nheappush(PQ, (dist[neighbor], neighbor))\nreturn dist\n```\nUnfortunately, eager Dijkstra is not so easy to implement in Python because we are missing the `decrease\\_key()` operation in a heap (and Python does have a self-balancing BST data structure, which can also be used for eager Dijkstra). You can see a BST-based C++ implementation in my other blog post.\nThe `dijkstra\\_lazy()` algorithm above is more or less standard and it has been known as \"lazy Dijkstra\" for a while. However, it is possible to make an even lazier version which has the same runtime and space analysis (but likely bigger constant factors). The idea is that instead of only adding to the PQ the neighbors for whom we find an improved distance, we can simply add all of them, and discard duplicates once we extract them from the PQ:\n```\ndef dijkstra\\_super\\_lazy(adj\\_lists, start):\ndist = defaultdict(int)\ndist[start] = 0\nPQ = [(0, s)]\nwhile PQ:\nd, node = heappop(PQ)\nif dist[node] != math.inf: continue\ndist[node] = d\nfor neighbor, weight in adj\\_lists[node]:\nheappush(PQ, (dist[node]+weight, neighbor))\nreturn dist\n```\n## So, Lazy or Eager?\n\nWe could keep looking at lazy vs eager algorithms, but I'll stop here. In aggregate, these are the pros and cons that I see:\n### Pros of lazy algorithms\n\n- \\*\\*Lazy algorithms require less code.\\*\\* This is because you only need to validate the parameters of the recursive function once at the beginning, instead of validating what you pass to each recursive call. This is specially true in binary tree problems, where you usually have two recursive calls. It doesn't apply as much for graphs.\n\n- \\*\\*Lazy algorithms require less indentation.\\*\\* For instance, in graph problems, we don't need to do checks inside the for loop over the neighbors.\n\n- \\*\\*Lazy algorithms do not require special handling for the first recursive call.\\*\\* You don't need to worry about things like checking if the root is NULL or marking the start node as visited.\n\n- \\*\\*Lazy recursive functions have simpler preconditions.\\*\\* You can just pass anything to them, and they work.\n### Pros of eager algorithms\n\n- \\*\\*For a graph BFS, eager has a better space complexity.\\*\\* This is a case where eager is objectively better. (Eager Dijkstra is also better but it is not expected to be implemented in interviews. Your interviewer is probably expecting lazy Dijkstra.)\n\n- \\*\\*Eager algorithms do fewer recursive calls or iterations.\\*\\* In a binary tree, the number of NULL nodes is always one more than the number of internal nodes. This means that a lazy traversal does twice as many recursive calls/iterations as the eager counterpart. This could make a big difference if you want to debug the code manually. For instance, in this picture, you can see that adding NULLs to the queue makes visualizing the steps more painful:\n![Evolution of the queue during level-order traversal](/blog/lazy-vs-eager/levelorder.png)\n\n- \\*\\*Eager algorithm can 'feel safer'.\\*\\* A friend commented that, with a lazy algorithm, they feel like they are missing an edge case.\n### My preference\n\nHere are my personal preferences for coding interviews (not those of the other authors of 'Beyond Cracking the Coding Interview'):\n\\*\\*Strong preferences:\\*\\*\n\n- For BFS, use eager. This one is clear cut.\n\n- For memoization DP, use lazy. It is much cleaner to code.\n\n- For Dijkstra, use lazy Dijkstra (not super lazy Dijkstra). It is what is realistic to do in an interview and probably what the interviewer expects.\n\\*\\*Weak preferences:\\*\\*\n\n- For binary tree traversals (iterative or recursive), use lazy. It is a bit cleaner.\n\n- For graph DFS, use eager. It is a bit more standard, and aligned with a graph BFS.\nIn the book, we'll definitely mention that some algorithms can be implemented in a lazy or eager way (in way less detail than here), and that you should choose the one that feels easier to you. But, we still need to pick one to show in the problem solutions. One idea is trying to be consistent throughout (e.g., doing all tree and graph traversals in an eager way). If you have an opinion on which one is better, please reach out! I'd love to hear it.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/lazy-vs-eager?category=dsa"
    },
    {
      "title": "Actually Implementing Dijkstra's Algorithm",
      "content": "# Actually Implementing Dijkstra's Algorithm\n\nJune 22, 2020\n![Actually Implementing Dijkstra's Algorithm](/blog/implementing-dijkstra/cover.png)\n## Introduction\n\nDijkstra's algorithm for the shortest-path problem is one of the most important graph algorithms, so it is often covered in algorithm classes. However, going from the pseudocode to an actual implementation is made difficult by the fact that it relies on a priority queue with a \"decrease key\" operation. While most programming languages offer a priority queue data structure as part of their standard library, this operation is generally not supported (e.g., in C++, Java or Python). In this blog, we go over the different ways to implement Dijkstra's algorithm with and without this operation, and the implications of using each. All in all, we consider 5 versions of Dijkstra (names mostly made up by me):\n\n- \\*\\*Textbook Dijkstra\\*\\*: the version commonly taught in textbooks where we assume that we have a priority queue with the \"decrease key\" operation. As we said, this often does not hold true in reality.\n\n- \\*\\*Linear-search Dijkstra\\*\\*: the most naive implementation, but which is actually optimal for dense graphs.\n\n- \\*\\*Lazy Dijkstra\\*\\*: practical version which does not use the \"decrease key\" operation at all, at the cost of using some extra space.\n\n- \\*\\*BST Dijkstra\\*\\*: version which uses a self-balancing binary search tree to implement the priority queue functionality, including the \"decrease key\" operation.\n\n- \\*\\*Theoretical Dijkstra\\*\\*: version that uses a Fibonacci heap for the priority queue in order to achieve the fastest possible runtime in terms of big-O notation. This is actually impractical due to the complexity and high constant factors of the Fibonacci heap.\nRoughly, each of the 5 versions corresponds to a different data structure used to implement the priority queue. Throughout the post, let `n` be the number of nodes and `m` the number of edges. Here is summary of the resulting runtime and space complexities:\n\n- \\*\\*Textbook Dijkstra\\*\\*: indexed binary heap. Runtime: `O(m\\*log n)`; space: `O(n)`.\n\n- \\*\\*Linear-search Dijkstra\\*\\*: unordered array. Runtime: `O(n²)`; space: `O(n)`.\n\n- \\*\\*Lazy Dijkstra\\*\\*: binary heap. Runtime: `O(m\\*log n)`; space: `O(m)`.\n\n- \\*\\*BST Dijkstra\\*\\*: self-balancing BST. Runtime: `O(m\\*log n)`; space: `O(n)`.\n\n- \\*\\*Theoretical Dijkstra\\*\\*: Fibonacci heap. Runtime: `O(m + n\\*log n)`; space: `O(n)`.\nWe provide implementations in Python and C++. The initial sections are mostly background. If you are already familiar with Dijkstra's algorithm, you can skip to the code snippets.\n## The shortest-path problem\n\nThe input consists of a graph `G` and a special node `s`. The edges of `G` are directed and have non-negative weights. The edge weights represent the \"lengths\" of the edges. The goal is to find the distance from `s` to every other node in `G`. The distance from `s` to another node is the length of the shortest path from `s` to that node, and the length of a path is the sum of the lengths of its edges. If a node is unreachable from `s`, then we say that the distance is infinite.\nMore precisely, this is known as the \"single-source shortest-path\" (SSSP) problem, because we find the distance from one node to every other node. Related problems include the \"all-pairs shortest paths\" problem and the single-source single-destination problem. Dijkstra's algorithm is a really efficient algorithm for the SSSP problem when the edges are non-negative. Dijkstra's algorithm does not work in the presence of negative edges (zero-weight edges are fine). If `G` contains negative edges, we should use the Bellman-Ford algorithm instead.\nThe constraint that the edges are directed is not important: if `G` is undirected, we can simply replace every undirected edge `{u,v}` with a pair of directed edges `(u,v)` and `(v,u)` in opposite directions and with the weight of the original edge.\nTo simplify things, we make a couple of assumptions that do not make any actual difference:\n\n- Nodes not reachable by `s` play no role in the algorithm, so we assume that `s` can reach every node. This is so that, in the analysis, we can assume that `n=O(m)`.\n\n- We assume that the distance from `s` to every node is unique. This allows us to talk about \"the\" shortest path to a node, when in general there could be many.\n## The graph's representation\n\nA graph is a mathematical concept. In the context of graph algorithms, we need to specify how the graph is represented as a data structure. For Dijkstra's algorithm, the most convenient representation is the adjacency list. The valuable thing about the adjacency list representation is that it allows us to iterate through the out-going edges of a node efficiently.\nIn the version of the adjacency list that we use, each node is identified with an index from `0` to `n-1`. The adjacency list contains one list for each node. For each node `u` between `0` and `n-1`, the list `G[u]` contains one entry for each neighbor of `u`. In a directed graph, if we have an edge `(u,v)` from `u` to `v`, we say that `v` is a neighbor of `u`, but `u` is not a neighbor of `v`. Since the graph is weighted, the entry for each neighbor `v` consists of a pair of values, `(v, l)`: the destination node `v`, and the length `l` of the edge `(u,v)`.\n## Dijkstra's algorithm idea\n\nOne of the data structures that we maintain is a list `dist` where `dist[u]` is the best distance known for `u` so far. At the beginning, `dist[s] = 0`, and for every other node `dist[u] = infinity`. These distances improve during the algorithm as we consider new paths. Our goal is to get to the point where `dist` contains the correct distance for every node.\nDuring the algorithm, the `dist` list is only updated through an operation called \"relaxing\" an edge.\n```\ndef relax(u,v,l): #l is the length of the edge (u,v)\n\nif dist[u] + l < dist[v]:\ndist[v] = dist[u] + l\n```\nIn words, relaxing an edge `(u,v)` means checking if going to `u` first and then using the edge `(u,v)` is shorter than the best distance known for `v`. If it is shorter, then we update `dist[v]` to the new, better value.\nDijkstra's algorithm is based on the following observations:\n\n- if `dist[u]` is correct \\*\\*and\\*\\* the shortest path from `s` to `v` ends in the edge `(u,v)`, then if we relax the edge `(u,v)`, we will find the correct distance to `v`. If either of the conditions are not satisfied, relaxing `(u,v)` may improve `dist[v]`, but it will not be the correct distance.\n\n- To find the correct distance to `v`, we need to relax all the edges in the shortest path from `s` to `v`, in order. If we do it in order, each node in the path will have the correct distance when we relax the edge to the next node, satisfying the conditions.\nDijkstra's algorithm is efficient because every edge is relaxed only once (unlike other algorithms like Bellman-Ford, which relaxes the edges multiple times). To relax every edge only once, we must relax the out-going edges of each node only after we have found the correct distance for that node.\nAt the beginning, only `s` has the correct distance, so we relax its edges. This updates the entries in `dist` for its neighbors. The neighbor of `s` that is closest to `s`, say, `x`, has the correct distance at this point. This is because every other path from `s` to `x` starts with a longer edge, and, since the graph does not have negative-weight edges, additional edges can only increase the distance. Next, since `x` has the correct distance, we can relax its out-going edges. After that, the node `y` with the 3rd smallest distance in `dist` (after `s` and `x`) has the correct distance because the node before `y` in the shortest path from `s` to `y` must be either `s` or `x`. It cannot be any other node because simply reaching any node that is not `s` or `x` is already more expensive than the distance we have found for `y`. We continue relaxing the out-going edges of nodes, always taking the next node with the smallest found distance. By generalizing the argument above, when we relax the out-going edges of each node, that node already has the correct distance. We finish after we have gone through all the nodes. At that point, `dist` contains the correct distance for every node.\n```\nHigh-level pseudocode of Dijkstra's algorithm\ndijkstra(G, s):\ndist = list of length n initialized with INF everywhere except for a 0 at position s\nmark every node as unvisited\nwhile there are unvisited nodes:\nu = unvisited node with smallest distance in dist\nmark u as visited\nfor each edge (u,v):\nrelax((u,v))\n```\nIn order to implement Dijkstra's algorithm, we need to decide the data structures used to find the unvisited node with the smallest distance at each iteration.\n## Priority queues\n\nPriority queues are data structures that are useful in many applications, including Dijkstra's algorithm.\nIn a normal queue, we can insert new elements and extract the oldest element. A priority queue is similar, but we can associate a priority with each element. Then, instead of extracting the oldest element, we extract the one with highest priority. Depending on the context, \"highest priority\" can mean the element with the smallest or largest priority value. In this context, we will consider that the highest priority is the element with the smallest priority value.\nA priority queue is an \\*abstract\\* data structure. That means that it only specifies which operations it supports, but not how they are implemented. There actually exist many ways to implement a priority queue. To make matters more confusing, different priority queues implementations support different sets of operations. The only agreed part is that they must support two basic operations:\n\n- `insert(e, k)`: insert element `e` with priority `k`.\n\n- `extract\\_min()`: remove and return the element with the smallest priority value.\nFor Dijkstra's algorithm, we can use a priority queue to maintain the nodes, using `dist[u]` as the priority for a node `u`. Then, at each iteration we can extract the unvisited node with the smallest distance. However, there is a problem: when we relax an edge, the value `dist[u]` may decrease. Thus, we need the priority queue to support a third operation which is not commonly supported:\n\n- `change\\_priority(e, k)`: set the priority of `e` to `k` (assuming that `e` is in the priority queue).\nA related operation is removing elements that are not the highest priority:\n\n- `remove(e)`: remove `e` (assuming that `e` is in the priority queue).\nIf a priority queue implements remove, we can use it to obtain the same functionality as `change-priority(e, k)`: we can first call `remove(e)` and then reinsert the element with the new key by calling `insert(e, k)`.\n## Pseudocode with a priority queue\n\nAssuming that we have a priority queue data structure that supports `insert`, `extract-min`, and `change-priority`, Dijkstra's pseudocode would be as follows.\nThe priority queue contains the unvisited nodes, prioritized by distance from `s`. At the beginning, the priority queue contains all the nodes, and they are removed as they are visited.\n```\nDijkstra pseudocode (with a priority queue)\ndijkstra(G, s):\ndist = list of length n initialized with INF everywhere except for a 0 at position s\nPQ = empty priority queue\nfor each node u: PQ.insert(u, dist[u])\nwhile not PQ.empty():\nu = PQ.extract\\_min()\nfor each edge (u,v) of length l:\nif dist[u]+l < dist[v]:\ndist[v] = dist[u]+l\nPQ.change\\_priority(v, dist[v])\n```\nA common variation is to add them to the priority queue when they are reached for the first time, instead of adding all the nodes at the beginning. The only change is how the priority queue is initialized and the if-else cases at the end:\n```\nDijkstra pseudocode (with deferred insertions to the PQ)\ndijkstra(G, s):\ndist = list of length n initialized with INF everywhere except for a 0 at position s\nPQ = empty priority queue\nPQ.insert(s, 0)\nwhile not PQ.empty():\nu = PQ.extract\\_min()\nfor each edge (u,v) of length l:\nif dist[u]+l < dist[v]:\ndist[v] = dist[u]+l\nif v in PQ: PQ.change\\_priority(v, dist[v])\nelse: PQ.insert(v, dist[v])\n```\nIt does not change the runtime or space complexity, but there is also no downside to deferring insertions to the PQ. On average, the PQ will contains fewer elements.\n## Analysis of Dijkstra's algorithm\n\nUsually, we analyze the algorithms \\*after\\* implementing them. However, in order to choose the best data structure for the priority queue, we need to analyze how much we use each type of operation.\nThus, it is convenient to define the runtime in terms of the priority queue operations, without specifying yet how they are done. Let `T\\_ins`, `T\\_min`, and `T\\_change` be the time per `insert`, `extract\\_min`, and `change\\_priority` operation, respectively, on a priority queue containing `n` elements.\nThe main `while` loop has `n` iterations, and the total number of iterations of the inner `for` loop, across all `n` iterations, is `m`. This is because each edge is relaxed once.\nThe runtime is dominated by the priority queue operations, so it is `O(n\\*T\\_ins + n\\*T\\_min + m\\*T\\_change)`. These operations dominate the runtime because everything else combined (like updating the `dist` list) takes `O(n+m)` time.\n## Linear-search Dijkstra for dense graphs\n\nThe simplest way to simulate the `extract\\_min` functionality of a priority queue is to iterate through the entire `dist` list to find the smallest value among the non-visited entries. If we do this, we don't need a priority queue. We call this \\*\\*linear-search Dijkstra\\*\\*. We get `T\\_ins = O(1)`, `T\\_min = O(n)`, and `T\\_change = O(1)`. Plugging those in, the total runtime of linear-search Dijkstra is `O(n + n\\*n + m) = O(n²)`, where we simplify out the `m` term because `n² > m` in any graph. More precisely, a directed graph with `n` nodes has at most `n\\*(n-1)=O(n²)` edges.\nA graph with \"close to\" `n\\*(n-1)` edges is called dense. \\*\\*Linear-search Dijkstra is actually optimal for dense graphs.\\*\\* This is because Dijkstra's algorithm must take `O(m)` time just to relax all edges, so it cannot be faster than `O(m)`, and, in dense graphs that is already proportional to `O(n²)`.\nHere is a Python implementation:\n```\ndef linearSearchDijkstra(G, s):\nn = len(G)\nINF = 9999999\ndist = [INF for node in range(n)]\ndist[s] = 0\nvis = [False for node in range(n)]\nfor i in range(n):\nu = -1\nfor v in range(n):\nif not vis[v] and (u == -1 or dist[v] < dist[u]):\nu = v\nif dist[u] == INF: break #no more reachable nodes\n\nvis[u] = True\nfor v, l in G[u]:\nif dist[u] + l < dist[v]:\ndist[v] = dist[u] + l\nreturn dist\n```\nAnd C++. We omit the includes and \"`using namespace std;`\".\n```\nvector linearSearchDijkstra(const vector>>& G, int s) {\nint n = G.size();\nvector dist(n, INT\\_MAX);\ndist[s] = 0;\nvector vis(n, false);\nfor (int i = 0; i < n; i++) {\nint u = -1;\nfor (int v = 0; v < n; v++)\nif (not vis[v] and (u == -1 or dist[v] < dist[u]))\nu = v;\nif (dist[u] == INT\\_MAX) break; //no more reachable nodes\nvis[u] = true;\nfor (auto edge : G[u]) {\nint v = edge.first, l = edge.second;\nif (dist[u]+l < dist[v])\ndist[v] = dist[u]+l;\n}\n}\nreturn dist;\n}\n```\n## Priority queues for sparse graphs\n\nThe `O(n²)` time from the implementation above is slow if the graph `G` is sparse, meaning that the number of edges is small relative to `O(n²)`. Recall that the time is `O(n\\*T\\_ins + n\\*T\\_min + m\\*T\\_change)`. If `m` is more similar to `n` than to `n²`, then we would be happy to trade a slower `change\\_priority` time for a faster `extract\\_min` time.\nThe best possible answer in terms of big-O notation is to use a priority queue implementation based on a data structure known as a \\*\\*Fibonacci Heap\\*\\*. A Fibonacci heap containing at most `n` elements achieves the following times:\n\n- `insert`: `O(log n)` amortized time.\n\n- `extract\\_min`: `O(log n)` amortized time.\n\n- `change\\_priority`: `O(1)` amortized time.\nAmortized time means that it could take more time, but, if we average out the times for that operation across the execution of an algorithm, each one takes that time on average.\nUsing a Fibonacci heap, we get a total time of `O(n\\*log n + m)` for Dijkstra's algorithm. This is really fast in terms of big-O notation, but Fibonacci heaps have larger constant factors than other data structures, making them slower in practice.\nThe most common way to implement a priority queue is with a \\*\\*binary heap\\*\\*. It is simple and fast in practice. Binary heaps support `insert` and `extract\\_min` in `O(log n)` like a Fibonacci heap. However, they do not support the `change\\_priority` operation.\nIt is possible to modify a binary heap to support the `change\\_priority` operation in `O(log n)` time. The result is sometimes called an \"indexed priority queue\". Using an indexed priority queue, we would get a total runtime of `O(n\\*log n + m\\*log n) = O(m\\*log n)`. This is slightly worse than with a Fibonacci heap, and faster in practice.\nIn any case, the priority queues provided by languages like C++, Python, and Java, do not support the `change\\_priority` operation. This creates a disconnect between the pseudocode taught in classrooms and the actual code that we can write.\nThe goal of this post is to illustrate the options to deal with this issue. There are 3:\n\n- \\*\\*Textbook Dijkstra\\*\\*: find or implement our own indexed priority queue.\n\n- \\*\\*Lazy Dijkstra\\*\\*: we implement Dijkstra without using the `change\\_priority` operation at all.\n\n- \\*\\*BST Dijkstra\\*\\*: we use a self-balancing binary search tree as the priority queue.\nWe will cover the latter two options. The first option is an interesting exercise in data structures (I implemented it once for a project), but it is more about the inner workings of binary heaps than it is about Dijkstra's algorithm.\nAll three options have a runtime of `O(m\\*log n)`. Note that for dense graphs, this becomes `O(n² log n)` time, so they are all worse than the naive linear-search Dijkstra. In terms of space, lazy Dijkstra is worse than the others, as it needs `O(m)` space, as opposed to `O(n)` for the other options.\n## Lazy Dijkstra\n\nWe implement Dijkstra using a priority queue that does not support the change-priority operation. We need the following change: when we find a shorter distance to a node that is already in the priority-queue, instead of using the \"change-priority\" operation, we simply use an \"insert\" operation and add a copy of the node in the priority queue with the new distance. Then, when we extract a node from the priority queue, we ignore it if it is not the first time we extract that node. We call this version of Dijkstra \"lazy Dijkstra\" because we \"postpone\" the removal of the pre-existing copy of the node.\nHere is a Python version. The logical structure of a binary heap is a binary tree, but, internally the tree is represented as an array for efficiency reasons. Python is a bit whack because, instead of having a priority queue module that encapsulates the implementation, we have the heapq module, which provides priority queue operations that can be used directly on a list representing a binary heap. `heapq` offers functions `heappop` (equivalent to `extract\\_min`) and `heappush` (equivalent to `insert`). These functions receive a normal Python list as a parameter, and this list is assumed to represent a binary heap. In Python, if the priority queue contains tuples, then the first element in the tuple is the priority. Thus, in the implementation we insert tuples to the priority queue with the distance first and the node second.\n```\ndef lazyDijkstra(G, s):\nn = len(G)\nINF = 9999999\ndist = [INF for u in range(n)]\ndist[s] = 0\nvis = [False for u in range(n)]\nPQ = [(0, s)]\nwhile len(PQ) > 0:\n\\_, u = heappop(PQ) #only need the node, not the distance\n\nif vis[u]: continue #not first extraction\n\nvis[u] = True\nfor v, l in G[u]:\nif dist[u]+l < dist[v]:\ndist[v] = dist[u]+l\nheappush(PQ, (dist[u]+l, v))\nreturn dist\n```\nHere is a C++ version:\n```\nvector lazyDijkstra(const vector>>& G, int s) {\nint n = G.size();\nvector dist(n, INT\\_MAX);\ndist[s] = 0;\nvector vis(n, false);\n//PQ of (distance, node) pairs prioritized by smallest distance\npriority\\_queue, vector>, greater>> PQ;\nPQ.push({0, s});\nwhile (not PQ.empty()) {\nint u = PQ.top().second;\nPQ.pop();\nif (vis[u]) continue; //not first extraction\nvis[u] = true;\nfor (auto edge : G[u]) {\nint v = edge.first, l = edge.second;\nif (dist[u]+l < dist[v]) {\ndist[v] = dist[u]+l;\nPQ.push({dist[v], v});\n}\n}\n}\nreturn dist;\n}\n```\nAnalysis: since nodes can be added to the priority queue multiple times, in lazy Dijkstra the maximum number of elements in the priority queue increases from `O(n)` to `O(m)`. As a result, we do `O(m)` `extract\\_min` and `insert` operations. The total runtime is `O(m\\*log m)`. This can be simplified to `O(m\\*log n)`, because `log m < log (n²) = 2 log n = O(log n)`. Thus, in terms of big-O notation, \\*\\*lazy Dijkstra is equally fast as textbook Dijkstra\\*\\* (Dijkstra with an indexed priority queue). The only thing that got worse is the space used by the priority queue.\n## BST Dijkstra\n\nSelf-balancing binary search trees, like red-black trees or AVL trees, are a type of data structure that maintains a set of elements ordered according to values associated with the elements, known as the elements' keys. They support a few operations, all in `O(log n)` time. For our use case, we are interested in the following ones:\n\n- Insert an element with a given key.\n\n- Find the element with the smallest/largest key.\n\n- Given a key, find if there is an element with that key, and optionally remove it.\nThese operations allow us to use a self-balancing BST to implement a priority queue. With the third operation, we can even implement the `change\\_priority` operation, as we mentioned.\nPython does not actually have a self-balancing binary search tree module (why?!), so we cannot implement this version of Dijkstra either without finding or implementing our own self-balancing BST.\nHere is a C++ version. In C++, the set data structure is implemented as a self-balancing BST:\n```\nvector bstDijkstra(const vector>>& G, int s) {\nint n = G.size();\nvector dist (n, INT\\_MAX);\ndist[s] = 0;\n//self-balancing BST of (distance, node) pairs, sorted by smallest distance\nset> PQ;\nPQ.insert({0, s});\nwhile (not PQ.empty()) {\nint u = PQ.begin()->second; //extract-min\nPQ.erase(PQ.begin());\nfor (auto edge : G[u]) {\nint v = edge.first, l = edge.second;\nif (dist[u]+l < dist[v]) {\n//erase and insert instead of change-priority\nPQ.erase({dist[v], v});\ndist[v] = dist[u]+l;\nPQ.insert({dist[v], v});\n}\n}\n}\nreturn dist;\n}\n```\nAnalysis: in a sense, BST Dijkstra combines the best of both worlds: it has the same runtime and space complexity as textbook Dijkstra, without needing the extra space of Lazy Dijkstra, but it uses a much more ubiquitous data structure, a self-balancing BST. However, in practice, self-balancing BSTs are slower than binary heaps. This has to do with the fact that heaps can be implemented on top of an array, while BSTs use recursive tree data structures with child pointers. The array has much better locality of reference. For sparse graphs, I'd expect the performance of the different versions to be ordered as follows:\nTextbook Dijkstra > Lazy Dijkstra > BST Dijkstra > Theoretical Dijkstra > Linear-search Dijkstra\n## Practice problems\n\nHere are some problems on leetcode:\n\n- Network Delay Time\n\n- Find the City With the Smallest Number of Neighbors at a Threshold Distance\n\n- Reachable Nodes In Subdivided Graph\n\n- Path with Maximum Minimum Value (Premium only)",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/implementing-dijkstra?category=dsa"
    },
    {
      "title": "Reachability Problems and DFS",
      "content": "# Reachability Problems and DFS\n\nJune 21, 2020\n![Reachability Problems and DFS](/blog/reachability-problems-and-dfs/cover.png)\n## Introduction\n\nDepth-first search, or DFS, is a fundamental graph algorithm that can be used to solve \\*\\*reachability\\*\\* problems. This post shows how to adapt the basic DFS template to solve several problems of this kind. Reachability problems are often easier in undirected graphs. Below, we specify if the algorithm works for undirected graphs, directed graphs, or both.\n### Prerequisites\n\nWe assume that the reader is already familiar with the concept of DFS. Here is an excellent video introducing DFS with step-by-step animations. We also assume that the reader is familiar with the adjacency list representation of a graph, and we use big-O notation in the analysis.\n### Coding conventions\n\nThe algorithms below are in Python. `n` denotes the number of nodes. Nodes are identified with integers in the range `0..n-1`. The graph `G` is a graph stored as an adjacency list: `G` is a list of `n` lists. For each `v` between `0` and `n-1`, `G[v]` is the list of neighbors of `G`.\nIf the graph is given as an edge list instead, we can initialize it as follows:\n```\ndef makeAdjList(edgeList):\nn = max(max(edge) for edge in edgeList) + 1\nG = [[] for v in range(n)]\nfor u,v in edgeList:\nG[u].append(v)\nG[v].append(u) #omit this for directed graphs\n\nreturn G\n```\nIf the graph is given as an adjacency matrix, we can iterate through the rows of the adjacency matrix instead of through the adjacency lists. To iterate through the neighbors of a node `v`, instead of\n```\nfor u in G[v]:\n#u is a neighbor of v\n\n...\n```\nwe do\n```\nfor u in range(n):\nif adjMatrix[v][u]:\n#u is a neighbor of v\n\n...\n```\nNote that using an adjacency matrix affects the runtime analysis of DFS: `O(n²)` instead of `O(m)`.\n## Which nodes can be reached from node s?\n\nThis is the simplest question that can be answered with DFS. The primary data structure in DFS is a list of booleans to keep track of already visited nodes (we call it `vis`). If we start a DFS search from a node `s`, the reachable nodes will be the ones for which `vis` is true.\nFor this, `G` can be directed or undirected. We make use of a nested function in Python so that we do not need to pass `G` and `vis` as parameters (in Python nested functions have visibility over the variables in the scope where they are defined).\n```\ndef reachableNodes(G, s): #G is directed or undirected\n\nn = len(G)\nvis = n \\* [False]\nvis[s] = True\n#invariant: v is marked as visited when calling visit(v)\n\ndef visit(v):\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nvisit(nbr)\nvisit(s)\nreturn [v for v in range(n) if vis[v]]\n```\nDFS runs in `O(m)` time and `O(n)` space, where `m` is the number of edges. This is because each edge is considered twice, once from each endpoint, if the endpoints end up being visited, or zero times if the endpoints are not visited.\n### Iterative version\n\n```\ndef reachableNodes(G, s): #G is directed or undirected\n\nn = len(G)\nvis = n \\* [False]\nstk = [s]\n#mark nodes as visited when removed from the stack, not when added\n\nwhile stk:\nv = stk.pop()\nif vis[v]: continue\nvis[v] = True\nfor nbr in G[v]:\nif not vis[nbr]:\nstk.append(nbr)\nreturn [v for v in range(n) if vis[v]]\n```\nThe iterative version takes `O(m)` space instead of `O(n)` because nodes can be inserted into the stack multiple times (up to one time for each incident edge). Alternatively, we can mark the nodes as visited when we add them to the stack instead of when we remove them. This change reduces the space usage to the usual `O(n)`. However, with this change, the algorithm is no longer DFS. It still works for answering reachability questions because the set visited nodes is the same, but the order in which they are visited is no longer consistent with a depth-first search order (it is closer to a BFS (breath-first search) order, but also not exactly a BFS order).\nThe difference between marking nodes when they added vs removed from the stack is discussed in detail here. Since the recursive version is shorter and optimal in terms of space, we favor it from now on. That said, it should be easy to adapt the iterative version above to the problems below.\n## Can node s reach node t?\n\nWe use the same code from before, but we add early termination as soon as we see `t`. Now, the recursive function has a return value.\n```\ndef canReachNode(G, s, t): #G is directed or undirected\n\nn = len(G)\nvis = n \\* [False]\nvis[s] = True\n#returns True if the search reaches t\n\ndef visit(v):\nif v == t: return True\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nif visit(nbr): return True\nreturn False\nreturn visit(s)\n```\nAdding the early termination can make the DFS faster, but in the worst-case the time/space complexity is the same.\n#### Practice problems\n\n- https://leetcode.com/problems/the-maze/\nThe hardest part on this problem is constructing the graph in the first place.\n## Find a path from s to t\n\nThe edges \"traversed\" in a DFS search form a tree called the \"DFS tree\". The DFS tree changes depending on where we start the search. The starting node is called the root. We can construct the DFS tree by keeping track of the predecessor of each node in the search (the root has no predecessor). If we construct the DFS tree rooted at `s`, we can follow the sequence of predecessors from `t` to `s` to find a path from `s` to `t` in reverse order.\nInstead of using the list `vis` to keep track of visited nodes, we know a node is unvisited if it has no predecessor yet. We indicate that a node has no predecessor with the special value `-1`.\n```\ndef findPath(G, s, t): #G is directed or undirected\n\nn = len(G)\npred = n \\* [-1]\npred[s] = None\ndef visit(v):\nfor nbr in G[v]:\nif pred[nbr] == -1:\npred[nbr] = v\nvisit(nbr)\nvisit(s) #builds DFS tree\n\npath = [t]\nwhile path[-1] != s:\np = pred[path[-1]]\nif p == -1: return None #cannot reach t from s\n\npath.append(p)\npath.reverse()\nreturn path\n```\nNote that DFS does \\*not\\* find the shortest path form `s` to `t`. For that, we can use BFS (breath-first search). It just returns any path without repeated nodes.\n## Is the graph connected?\n\nFor undirected graphs, this is almost the same question as the first question (\"which nodes can be reached by `s`?\") because of the following property:\n\\*An undirected graph is connected if and only if every node can be reached from `s`, where `s` is any of the nodes.\\*\nThus, the code is exactly the same as for the first question, with two differences: 1) we choose `s` to be `0` (could be anything), and 2) we change the last line to check if every entry in `vis` is true.\n```\ndef isConnected(G): #G is undirected\n\nn = len(G)\nvis = n \\* [False]\nvis[0] = True\ndef visit(v):\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nvisit(nbr)\nvisit(0)\nreturn all(vis)\n```\nFor directed graphs, we need to take into account the direction of the edges. A directed graph is \\*\\*strongly connected\\*\\* if every node can reach every other node. We can use the following property:\n\\*A directed graph is strongly connected if and only if `s` can reach every node and every node can reach `s`, where `s` is any of the nodes.\\*\nWe already know how to use DFS to check if `s` can reach every node. To check if every node can reach `s`, we can do a DFS starting from `s`, \\*\\*but in the reverse graph of G\\*\\*. The reverse graph of `G` is like `G` but reversing the directions of all the edges.\n```\ndef isConnected(G): #G is directed\n\nn = len(G)\nvis = n \\* [False]\nvis[0] = True #use 0 for start node\n\ndef visit(G, v):\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nvisit(G, nbr)\nvisit(G, 0) #nodes reachable from s\n\nif not all(vis): return False\nGreverse = [[] for v in range(n)]\nfor v in range(n):\nfor nbr in G[v]:\nGreverse[nbr].append(v)\nvis = n \\* [False] #reset vis for the second search\n\nvis[0] = True\nvisit(Greverse, 0) #nodes that can reach s\n\nreturn all(vis)\n```\nThe runtime is still `O(m)`, but the space is now `O(m)` because we need to create and store the reverse graph. There are alternative algorithms (like Tarjan's algorithm) which can do this in `O(n)` space.\n## How many connected components are there?\n\nWe can use the typical DFS to answer this question for undirected graphs. We use a common pattern in DFS algorithms: an outer loop through all the nodes where we launch a search for every yet-unvisited node.\n```\ndef numConnectedComponents(G): #G is undirected\n\nn = len(G)\nvis = n \\* [False]\ndef visit(v):\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nvisit(nbr)\nnumCCs = 0\nfor v in range(n):\nif not vis[v]:\nnumCCs += 1\nvis[v] = True\nvisit(v)\nreturn numCCs\n```\nThe runtime is now `O(n+m)` because, if `m < n`, we still spend `O(n)` time iterating through the loop at the end.\nFor directed graphs, instead of connected components, we talk about \\*\\*strongly connected components\\*\\*. A strongly connected component is a maximal subset of nodes where every node can reach every other node.\nIf we want to find the number of strongly connected components, we can use something like Tarjan's algorithm, a DFS-based algorithm that requires some additional data structures.\n#### Practice problems\n\n- https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/ (Premium only)\n\n- https://leetcode.com/problems/number-of-islands/\n\n- https://leetcode.com/problems/friend-circles/\n## Which nodes are in the same connected components?\n\nThis question is more general than the previous two. We label each node `v` with a number `CC[v]` so that nodes with the same number belong to the same CC. Instead of having a list `CC` in addition to `vis`, we use the CC number `-1` to indicate unvisited nodes. This way, we do not need `vis`\n```\ndef connectedComponents(G): #G is undirected\n\nn = len(G)\nCC = n \\* [-1]\n##invariant: v is labeled with CC i>=0\n\ndef visit(v, i):\nfor nbr in G[v]:\nif CC[nbr] == -1:\nCC[nbr] = i\nvisit(nbr, i)\ni = 0\nfor v in range(n):\nif CC[v] == -1:\nCC[v] = i\nvisit(v, i)\ni += 1\nreturn CC\n```\nFor directed graphs, again we need Tarjan's algorithm or an equivalent algorithm.\n#### Practice problems\n\n- https://leetcode.com/problems/max-area-of-island/\n\n- https://leetcode.com/problems/sentence-similarity-ii/\nIn the second problem, nodes are given by names, not indices, so they need to be converted.\n## Is the graph acyclic?\n\nFor undirected graphs, this question is simple. First, we consider the problem in each CC independently. This is very common pattern in graph problems. We do this with an outer loop through all the nodes where we launch a search for every yet-unvisited node.\nDuring the DFS search in each CC, if we find an edge to an already visited node that is not the predecessor in the search (the node we just came from), there is a cycle. Such edges in a DFS search are called \\*\\*back edges\\*\\*. We add one parameter to the recursive function `visit` to know the predecessor node.\n```\ndef hasCycles(G): #G is undirected\n\nn = len(G)\nvis = n \\* [False]\n#returns True if the search finds a back edge\n\ndef visit(v, p):\nfor nbr in G[v]:\nif vis[nbr] and nbr != p: return True\nif not vis[nbr]:\nvis[nbr] = True\nif visit(nbr, v): return True\nreturn False\nfor v in range(n):\nif not vis[v]:\nvis[v] = True\n#the root of the search has no predecessor\n\nif visit(v, -1): return True\nreturn False\n```\nFor directed graphs, it is not as simple: the fact that a neighbor `nbr` is already visited during the DFS search does not mean that `nbr` can reach the current node. To check if a directed graph is acyclic, we can use the linear-time peel-off algorithm for finding a topological ordering. This algorithm detects if the graph is acyclic and finds a topological ordering if so, though we are only interested in the first part.\n#### Practice problems\n\n- https://leetcode.com/problems/redundant-connection/\nThis problem is easier to solve using union-find, but it can be done with DFS.\n## Is the graph a tree?\n\nUsually, we ask this question for undirected graphs. We can use this characterization of trees:\n\\*An undirected graph is a tree if and only if it is connected and has exactly `n-1` edges.\\*\nWe already saw how to check if the graph is connected with DFS, and counting the number of edges is straightforward:\n```\n#for undirected graphs:\n\nm = sum(len(G[v]) for v in range(n)) / 2\n#for directed graphs:\n\nm = sum(len(G[v]) for v in range(n))\n```\n#### Practice problems\n\n- https://leetcode.com/problems/graph-valid-tree/\n## Is the graph bipartite?\n\nThis is exactly the same question as whether the graph can be two-colored, so see the next section.\n## Can the graph be two-colored?\n\nTwo-coloring a graph means assigning colors to the nodes such that no two adjacent nodes have the same color, using only two colors. Usually, we consider coloring question for undirected graphs.\nWe consider whether each CC can be colored independently from the others. We can color each CC using DFS. We use values `0` and `1` for the colors. The color of the start node can be anything, so we set it to `0`. For the remaining nodes, the color has to be different from the parent, so we only have one option.\nInstead of having a `vis` array, we use the special color `-1` to denote unvisited nodes.\n```\ndef is2Colorable(G): #G is undirected\n\nn = len(G)\ncolor = n \\* [-1]\n#returns True if we can color all the nodes reached from v\n#invariant: v has an assigned color\n\ndef visit(v):\nfor nbr in G[v]:\nif color[nbr] == color[v]: return False\nif color[nbr] == -1:\ncolor[nbr] = 1 if color[v] == 0 else 0\nif not visit(nbr): return False\nreturn True\nfor v in range(n):\nif color[v] == -1:\ncolor[v] = 0\nif not visit(v): return False\nreturn True\n```\nWith 3 or more colors, the problem becomes a lot harder.\n#### Practice problems\n\n- https://leetcode.com/problems/is-graph-bipartite/\n## What is the distance from a node s to every other node in a tree?\n\nWe cannot use DFS to find the distance between nodes in a graph which can have cycles, because DFS is not guaranteed to follow the shortest path from the root to the other nodes. For that, BFS is more suitable (if the graph is unweighted). However, since trees are acyclic, there is a unique path between any two nodes, so DFS must use the unique path, which, by necessity, is the shortest path. Thus, we can use DFS to find distances in a tree.\n```\ndef getDistances(G, s): #G is undirected and a tree\n\nn = len(G)\ndists = n \\* [-1]\ndists[s] = 0\n#invariant: v has an assigned distance\n\ndef visit(v):\nfor nbr in G[v]:\n#check nbr is not the predecessor\n\nif dists[nbr] != -1: continue\ndists[nbr] = dists[v] + 1\nvisit(nbr)\nvisit(s)\nreturn dists\n```\n#### Practice problems\n\n- https://leetcode.com/problems/time-needed-to-inform-all-employees/\n## Find a spanning tree\n\nA spanning tree of a connected, undirected graph `G` is a subgraph which has the same nodes as `G` that is a tree.\nThe edges traversed by a DFS search on a connected graph form a spanning tree (sometimes called a DFS tree). Thus, we do DFS and add the traversed edges to the resulting tree.\n```\ndef spanningTree(G): #G is undirected and connected\n\nn = len(G)\nvis = n \\* [False]\nvis[0] = True\nT = [[] for v in range(n)]\ndef visit(v):\nfor nbr in G[v]:\nif not vis[nbr]:\nvis[nbr] = True\nT[v].append(nbr)\nT[nbr].append(v)\nvisit(nbr)\nvisit(0)\nreturn T\n```\n## Conclusions\n\nDFS has many uses. We showed how to make minor modifications to the DFS template to answer reachability and connectivity questions.\nAfter DFS, the next algorithm to learn would be BFS (breath-first search). Like DFS, it can answer reachability questions. On top of that, it can also answer questions about distance in undirected graphs.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/reachability-problems-and-dfs?category=dsa"
    },
    {
      "title": "Breaking Down Dynamic Programming",
      "content": "# Breaking Down Dynamic Programming\n\nFebruary 5, 2020\n![Breaking Down Dynamic Programming](/blog/breaking-down-dynamic-programming/cover.png)\nNote: the approach in this guide later became the foundation for the dynamic programming chapter in Beyond Cracking the Coding Interview.\n## Introduction\n\nWhen I was a TA for \"Algorithm Design and Analysis\", the students struggled with dynamic programming. To simplify/demystify it, I tried to break it down into a logical sequence of steps, each of which should not feel too intimidating on its own. This is explained in detail here. To complement the explanations, there are links to problems on leetcode.com, in case the reader wants to practice. The code snippets are in Python, but Leetcode accepts most popular languages.\n## Overview: Recursive vs Iterative DP\n\nIn short, dynamic programming (DP) is a technique for problems that seem hard to solve as a whole, but become easy if we know the solution to smaller subproblems. More technically, we can use it in problems where the (value of the) solution can be expressed as an equation which is a function of the input, and is expressed in terms of itself with smaller inputs. This is called a \\*recurrence equation\\*. The classic example is the Fibonacci recurrence: `Fib(n) = Fib(n-1) + Fib(n-2)`.\n\n- https://leetcode.com/problems/fibonacci-number/\nA recurrence equation can be translated into code:\n```\ndef Fib(n):\nif n == 0 or n == 1:\nreturn 1\nreturn Fib(n-1) + Fib(n-2)\n```\nHowever, the above function has an exponential runtime. A recursive function becomes exponential when it is possible to reach the same subcall through different execution paths. In the Fibonacci case, we have the following nested calls: `Fib(n) -> Fib(n-1) -> Fib(n-2)`, and `Fib(n) -> Fib(n-2)`. Since `Fib(n-2)` is called twice all the work from this call is duplicated, which in turn means that subcalls made from `Fib(n-2)` will start to duplicate and grow exponentially.\nDynamic programming is simply a workaround to this duplication issue. Instead of recomputing the solutions of the subproblems, we store them and then we recall them as needed. This guarantees that each subproblem is computed only once.\nThere are two main approaches for DP.\n### Recursive / Top-down DP\n\nWe start with the code which is a literal translation of the recurrence equation, but then we add a dictionary / hash table to store results.\n```\nmemo = {}\ndef Fib(n):\nif n == 0 or n == 1:\nreturn 1\nif n in memo:\nreturn memo[n]\nres = Fib(n-1) + Fib(n-2)\nmemo[n] = res\nreturn res\n```\nThere are three changes in the code above:\n1. declaring our dictionary for storing results, `memo` outside the recursive function (memo comes \"memorization\" or \"memoization\", a name used in the literature).\n2. before computing the result, we check if the solution has already been computed. This check can be done before or after the base case.\n3. before returning, we save the result in the `memo` table.\nUsing a memoization table in this way solves the inefficiency (we will go deeper into the analysis part later).\n### Iterative / Bottom-up DP\n\nInstead of starting from the largest input and recursively reaching smaller subproblems, we can directly compute the subproblems from smallest to largest. This way, we already have the solutions to the subproblems when we need them. For this approach, we change the dictionary for an array/vector, and we change recursive calls for a for loop.\n```\ndef Fib(n):\nif n == 0: return 1\nmemo = [0 for i in range(n+1)]\nmemo[0], memo[1] = 1, 1\nfor i in range(2, n+1):\nmemo[i] = memo[i-1] + memo[i-2]\nreturn memo[n]\n```\nMost problems can be solved with both recursive and iterative DP. Here are some considerations for how to choose:\n\n- Recursive DP matches the recurrence equation more directly, so it can be easier to implement.\n\n- Both have the same runtime complexity, but the recursive version will generally have larger constant factors due to all the recursive function calling and due to using a hash table instead of an array.\n\n- Iterative DP often allows for an optimization to reduce the space complexity (discussed later).\n## Recursive DP in 5 Steps\n\n1. Choose what your subproblems are.\n2. Find the recurrence equation.\n3. Translate the recurrence equation into recursive code.\n4. Add memoization.\n5. (Optional) Reconstruct the solution.\nWe already saw steps 1–4 with the Fibonacci example. Now, we will walk through all the steps in more detail using a more complicated example, the \\*longest common subsequence problem\\*:\nGiven two strings `s1` and `s2`, find the length of the longest string which is a subsequence of both `s1` and `s2`. A string `t` is a \\*subsequence\\* of a string `s` if every char in `t` appears \\*\\*in order\\*\\* in `s`, but are \\*\\*not necessarily contiguous\\*\\*. For example, `abc` is a subsequence of `axbyz`, but `ba` is not (do not confuse subsequence with substring or subset).\n\n- https://leetcode.com/problems/longest-common-subsequence/\nStep 1: choose our subproblems. This varies from problem to problem, but when the input to the problem is a string, a natural way to obtain smaller problems is to look at shorter strings. Here we can use as a subproblem a \\*prefix\\* of `s1` and a prefix of `s2`.\nSome notation: let `n` be the length of `s1` and `m` the length of `s2`. Let `LCS(i,j)` be the solution for the LCS problem for the prefix of `s1` of length `n` (`s1[0..i-1]`) and the prefix of `s2` of length `m` (`s2[0..j-1]`). Then, our goal is to find `LCS(n, m)`.\nStep 2: find the recurrence equation. Now we need to come up with an expression for `LCS(i,j)` as a function of `LCS` with smaller indices (as well as a base case). This is the hardest step of DP, and often it is here that we realize that we chose bad subproblems in Step 1. If that happens, hopefully we will discover some hint for what our subproblems should be.\nIn order to derive the recurrence equation for LCS, we need the following observation: if the two strings end with the same character `c`, then, to maximize the length of the subsequence, it is \"safe\" to add `c` to the subsequence. In contrast, if both strings end with different characters, then \\*at least\\* one of them cannot appear in the subsequence. The complication is that we do not know which one. Thus, instead of guessing, we can simply consider both options.\nThis observation yields the recurrence equation (excluding base case):\n```\nLCS(i, j) = 1 + LCS(i-1, j-1) if s[i] == s[j]\nmax(LCS(i, j-1), LCS(i-1, j)) otherwise\n```\nThis step is not intuitive at first, and requires practice. After having done a few problems, one starts to recognize the typical patterns in DP. For instance, using `max` among a set of options of which we do not know which one is the best is easily the most common pattern in DP.\nStep 3. Translate the recurrence equation into recursive code. This step is a very simple programming task. Pay attention to the base case.\n```\n#outer call:\n\nLCS(len(s1), len(s2))\ndef LCS(i, j):\nif i == 0 or j == 0:\nreturn 0\nif s1[i-1] == s2[j-1]:\nreturn 1 + LCS(i-1, j-1)\nelse:\nreturn max(LCS(i, j-1), LCS(i-1, j))\n```\nIf we draw the few first steps of the call graph, we will see that the same subproblem is reached twice. Thus, call graph blows up, leading to an exponential runtime.\nStep 4. Add memo table. This step should be automatic: one does not even need to understand the previous code in order to add the memo table.\n```\n#outer call:\n\nmemo = {}\nLCS(len(s1), len(s2))\ndef LCS(i, j):\nif i == 0 or j == 0:\nreturn 0\nif (i,j) in memo:\nreturn memo[(i,j)]\nif s1[i-1] == s2[j-1]:\nres = 1 + LCS(i-1, j-1)\nelse:\nres = max(LCS(i, j-1), LCS(i-1, j))\nmemo[(i,j)] = res\nreturn res\n```\nThe base case corresponds to when one of the strings is empty. The LCS of an empty string with another string is clearly an empty string.\nIncidentally, if we flip the check on the memo table, the code becomes a bit more streamlined (fewer lines + merging the two returns). I prefer this form (it does the same):\n```\ndef LCS(i, j):\nif i == 0 or j == 0:\nreturn 0\nif (i,j) not in memo:\nif s1[i-1] == s2[j-1]:\nmemo[(i,j)] = 1 + LCS(i-1, j-1)\nelse:\nmemo[(i,j)] = max(LCS(i, j-1), LCS(i-1, j))\nreturn memo[(i,j)]\n```\nWe have eliminated the exponential blowup. In general, DP algorithms can be analyzed as follows: # of distinct subproblems times time per subproblem excluding recursive calls. For LCS, we get `O(nm)\\*O(1)=O(nm)`.\n\nStep 5. Reconstruct the solution.\nWe used DP to compute the length of the LCS. What if we want to find the LCS itself?\nA naive way to do it would be to store the entire result of each subproblem in the memoization table instead of just its length. While this works, it is clear that it will require a lot of memory to store `O(nm)` strings of length `O(min(n,m))` each. We can do better.\nStep 5, \"Reconstruct the solution\", is how to reuse the table that we constructed in Step 4 to find the actual solution instead of just its length. I said that this step is optional because sometimes we just need the \\*value\\* of the solution, so there is no reconstruction needed.\nThe good news is that we do not need to modify the code that we already wrote in Step 4. The reconstruction is a separate step that comes after. In addition, the reconstruction step is very similar (follows the same set of cases) as the step of building the memo table. In short, we use the memo table as an \"oracle\" to guide us in our choices. Based on the values in the memo table, we know which option is better, so we know how to reconstruct the solution.\n```\n#outer calls\n\nmemo = {}\nn, m = len(s1), len(s2)\nLCS(n, m) #build memo table\n\nsol = reconstructLCS(n, m)\ndef reconstructLCS(i, j):\nif i == 0 or j == 0:\nreturn \"\"\nif s1[i-1] == s2[j-1]:\nreturn reconstructLCS(i-1, j-1) + s1[i-1]\nelif memo[(i-1,j)] >= memo[(i,j-1)]:\nreturn reconstructLCS(i-1, j)\nelse:\nreturn reconstructLCS(i, j-1)\n```\nIn the code above, first we run `LCS(n,m)` to fill the memo table. Then, we use it in the reconstruction. The condition `memo[(i-1,j)] >= memo[(i,j-1)]` tells us that we can obtain a longer or equal LCS by discarding a char from `s1` instead of from `s2`.\nNote that there is a single recursive call in the reconstruction function, so the complexity is just `O(n+m)`.\n## Iterative DP in 6 Steps\n\n1. Choose what your subproblems are.\n2. Find the recurrence equation.\n3. \\*\\*Design the memo table.\\*\\*\n4. \\*\\*Fill the memo table.\\*\\*\n5. (Optional) Reconstruct the solution.\n6. \\*\\*(Optional) Space optimization.\\*\\*\nThe new/different steps are highlighted. Step 3. is to design the layout of the table/matrix where we are going to store the subproblem solutions. There is no coding in this step. By \"design\", I mean making the following choices:\n\n- what are the dimensions of the table, and what does each index mean. Generally speaking, the table should have one dimension for each parameter of the recurrence equation. In the case of LCS, it will be a 2-dimensional table.\n\n- where are the base cases.\n\n- where is the cell with the final solution.\n\n- what is the ``dependence relationship'' between cells (which cells do you need in order to compute each cell).\n\n- which cells do not need to be filled (in the case of LCS, we need them all).\nHere is how I would lay out the table for LCS (you can find a different layout in the problems below):\n![LCS table](/blog/breaking-down-dynamic-programming/lcstable.svg)\nNext (Step 4), we fill the memo table with a nested for loop. If the layout is good, this should be easy. Before the main loop, we fill the base case entries. Then, we must make sure to iterate through the table in an order that respects the dependencies between cells. In the case of LCS, we can iterate both by rows or by columns.\nWe obtain the following algorithm:\n```\ndef LCS(s1, s2):\nn, m = len(s1), len(s2)\nmemo = [[0 for j in range(m+1)] for i in range(n+1)]\nfor i in range(1, n+1):\nfor j in range(1, m+1):\nif s1[i-1] == s2[j-1]:\nmemo[i][j] = 1 + memo[i-1][j-1]\nelse:\nmemo[i][j] = max(memo[i-1][j], memo[i][j-1])\nreturn memo[n][m]\n```\nIn the code above, the base case entries are filled implicitly when we initialize the table with zeros everywhere.\nIf we need to reconstruct the solution, we can do it in the same way as for the recursive DP. The only difference is that memo is a matrix instead of dictionary.\n### Space optimization\n\nClearly, the space complexity of iterative DP is the size of the DP table. Often, we can do better. The idea is to only store the already-computed table entries that we will use to compute future entries. For instance, in the case of Fibonacci, we do not need to create an entire array -- keeping the last two numbers suffice. In the case of a 2-dimensional DP table, if we are filling the DP table by rows and each cell only depends on the previous row, we only need to keep the last row (and similarly if we iterated by columns). Here is the final version for LCS where we improve the space complexity from `O(nm)` to `O(n+m)`:\n```\ndef LCS(s1, s2):\nn, m = len(s1), len(s2)\nlastRow = [0 for j in range(m+1)]\nfor i in range(1,n+1):\ncurRow = [0 for j in range(m+1)]\nfor j in range(1,m+1):\nif s1[i-1] == s2[j-1]:\ncurRow[j] = 1 + lastRow[j-1]\nelse:\ncurRow[j] = max(lastRow[j], curRow[j-1])\nlastRow = curRow\nreturn lastRow[m]\n```\nNote: this optimization is incompatible with reconstructing the solution, because that uses the entire table as an \"oracle\".\n## DP Patterns\n\nHere are some typical patterns:\n### Step 1 (The Subproblems) Patterns\n\n- If the input is a string or a list, the subproblems are usually prefixes or substrings/sublists, which can be specified as a pair of indices.\n\n- If the input is a number, the subproblems are usually smaller numbers.\n\n- Generally speaking, the number of subproblems will be linear or quadratic on the input size.\n### Step 2 (The Recurrence Equation) Patterns\n\n- Often, we use `max` or `min` to choose between options, or sum to aggregate subsolutions.\n\n- The number of subproblems is most often constant, but sometimes it is linear on the subproblem size. In the latter case, we use an inner loop to aggregate/choose the best solution.\n\n- Sometimes, the recurrence equation is not exactly for the original problem, but for a related but more constrained problem. See an example below, \"Longest Increasing Subsequence\".\n## Practice Problems\n\nHere are some practice problems showcasing the patterns mentioned above. Follow the Leetcode links for the statements and example inputs. I jump directly to the solutions. I'd recommend trying to solve the problems before checking them.\n\n- https://leetcode.com/problems/palindromic-substrings/\nHere, the goal is to count the number of substrings of a string `s` which are palindromic. There is a trivial `O(n³)` time solution without DP:\n```\ndef countSubstrings(s):\nn = len(s)\ncount = 0\nfor i in range(n):\nfor j in range(i, n):\nif isPalindrome(s[i:j+1]):\ncount += 1\nreturn count\n```\nWe can improve this to `O(n²)` with DP. The subproblems are all the substrings of `s`. Let `Pal(i, j)` be true iff `s[i..j]` is a palindrome. We have the following recurrence equation (excluding base cases):\n```\nPal(i, j) = false if s[i] != s[j],\nPAl(i, j) = Pal(i+1, j-1) otherwise\n```\nBased on this recurrence equation, we can design the following DP table:\n![Table for substring count](/blog/breaking-down-dynamic-programming/palindrometable.svg)\nThis type of \"diagonal\" DP tables are very common when the subproblems are substrings/sublists. In this case, the base cases are substrings of length 1 or 2. The goal is `Pal(0,n-1)`.\nGiven the dependency, the table can be filled by rows (starting from the last row), by columns (starting each column from the bottom), or by diagonals (i.e., from shortest to longest substrings). In the code below, I illustrate how to fill the table by diagonals.\n```\ndef countSubstrings(s):\nn = len(s)\nT = [[False for j in range(n)] for i in range(n)]\nfor i in range(n):\nT[i][i] = True\nfor i in range(n-1):\nT[i][i+1] = s[i] == s[i+1]\nfor size in range(2, n+1):\nfor i in range(0,n-size):\nj = i + size\nT[i][j] = s[i] == s[j] and T[i+1][j-1]\ncount = 0\nfor row in T:\nfor val in row:\nif val:\ncount += 1\nreturn count\n```\n\n- https://leetcode.com/problems/minimum-path-sum/\nHere, a subproblem can be a grid with reduced width and height. Let `T[i][j]` be the cheapest cost to reach cell `(i,j)`. The goal is to find `T[n-1][m-1]`, where `n` and `m` are the dimensions of the grid. The base case is when either `i` or `j` are zero, in which case we do not have any choices for how to get there. In the general case, we have the recurrence equation `T[i][j] = grid[i][j] + min(T[i-1][j], T[i][j-1])`: to get to `(i,j)`, we first need to get to either `(i-1,j)` or to `(i,j-1)`. We use `min` to choose the best of the two. We convert this into an iterative solution:\n```\ndef minPathSum(grid):\nn, m = len(grid), len(grid[0])\nT = [[0 for j in range(m)] for i in range(n)]\nT[0][0] = grid[0][0]\nfor i in range(1, n):\nT[i][0] = grid[i][0] + T[i-1][0]\nfor j in range(1, m):\nT[0][j] = grid[0][j] + T[0][j-1]\nfor i in range(1, n):\nfor j in range(1, m):\nT[i][j] = grid[i][j] + min(T[i-1][j], T[i][j-1])\nreturn T[n-1][m-1]\n```\n\n- https://leetcode.com/problems/unique-paths-ii/\nThis is similar to the previous problem, but we need to accumulate the solutions to the subproblems, instead of choosing between them. Problems about \\*counting\\* solutions can often be solved with DP.\n\n- https://leetcode.com/problems/longest-increasing-subsequence/\nThis problem will illustrate a new trick: if you cannot find a recurrence equation for the original problem, try to find one for a more restricted version of the problem which nevertheless you enough information to compute the original problem.\nHere, the input is a list `L` of numbers, and we need to find the length of the longest increasing subsequence (a subsequence does not need to be contiguous). Again, the subproblems correspond to prefixes of the list.\nLet `LIS(i)` be the solution for the prefix of length `i` (`L[0..i]`). The goal is to find `LIS(n-1)`, where `n` is the length of `L`.\nHowever, it is not easy to give a recurrence equation for `LIS(i)` as a function of smaller prefixes. In particular, \\*\\*the following is wrong\\*\\* (I will let the reader think why):\n```\nLIS(i) = LIS(i-1) + 1 if L[i] > L[i-1],\nLIS(i) = LIS(i-1) otherwise\n```\nThus, we actually give a recurrence equation for a slightly modified type of subproblems: let `LIS2(i)` be the length of the LIS \\*\\*ending at index i\\*\\*. This constraint makes it easier to give a recurrence equation:\n```\nLIS2(i) = 1 + max(LIS2(j)) over all j < i such that L[j] < L[i]\n```\nIn short, since we know that the LIS ends at `L[i]`, we consider all candidate predecessors, which are the numbers smaller than it, and get the best one by using `max`. Crucially, this recurrence works for `LIS2(i)` but not for `LIS(i)`.\nHere is a full solution:\n```\ndef LIS(L):\nn = len(L)\nT = [0 for i in range(n)]\nT[0] = 1\nfor i in range(1, n):\nT[i] = 1\nfor j in range(0, i):\nif L[j] < L[i]:\nT[i] = max(T[i], T[j] + 1)\nreturn max(T)\n```\nAt the end, we do not simply return `T[n-1]` because `T` is the table for `LCS2`, not `LCS`. We return `max(T)` because the LCS must end \\*somewhere\\*, so `LCS(n-1) = LCS2(j)` for some `j < n`.\nNote that the runtime is `O(n²)` even though the table has linear size. This is because we take linear time per subproblem.\n\n- https://leetcode.com/problems/number-of-longest-increasing-subsequence/\nA harder version of the previous problem. A similar approach works. First solve the LIS problem as before, and then do a second pass to count the solutions.\n\n- https://leetcode.com/problems/shortest-common-supersequence/\nThis problem is similar to LCS, and it requires reconstruction.\nI should mention that not \\*every\\* problem that can be solved with DP fits into the mold discussed above. Despite that, it should be a useful starting framework. Here are many more practice problems:\n\n- https://leetcode.com/tag/dynamic-programming/",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/breaking-down-dynamic-programming?category=dsa"
    },
    {
      "title": "Iterative Tree Traversals: A Practical Guide",
      "content": "# Iterative Tree Traversals: A Practical Guide\n\nFebruary 4, 2020\n![Iterative Tree Traversals: A Practical Guide](/blog/iterative-tree-traversals/cover.png)\n## Introduction\n\nI don't know how often tree traversals come up in actual software projects, but they are popular in coding interviews and competitive programming.\nIn this article, I share an approach for implementing tree traversal algorithms iteratively that I found to be simple to remember and implement, while being flexible enough to do anything that a recursive algorithm can (I also didn't like most suggestions I saw online). The main technique is given in section \"Iterative Postorder and Inorder Traversal\", but first I give some context. I also link to practice problems on leetcode.com for the reader to play with. I provide some solutions, but I suggest trying the problems out first. The code snippets are in C++, but leetcode accepts most languages.\n## What are Tree Traversals\n\nMathematically, trees are just connected acyclic graphs. However, in the context of tree traversals, we are usually working with \\*\\*rooted trees\\*\\* represented with a recursive structure such as the following (which is the default definition in Leetcode for binary trees). A leaf is a node with two null pointers as children:\n```\nstruct TreeNode {\nint val;\nTreeNode \\*left;\nTreeNode \\*right;\nTreeNode(int x) : val(x), left(NULL), right(NULL) {}\n};\n```\nA tree traversal is an algorithm that visits every node in a tree in a specific order (and does some computation with them, depending on the problem). For binary trees specifically, there are three important orders:\n\n- \\*\\*Preorder:\\*\\* root before children. As we will see, this is the simplest to implement.\n\n- \\*\\*Inorder:\\*\\* left child, then root, then right child. This traversal is most often used on \\*binary search trees\\* (BST). A BST is a rooted binary tree with the additional property that every node in the left subtree has a smaller value than the root, and every node in the right subtree has a larger value than the root. This traversal is called \"inorder\" because, when used on a BST, it will visit the nodes from smallest to largest.\n\n- \\*\\*Postorder:\\*\\* children before root. It comes up in problems where we have to aggregate information about the entire subtree rooted at each node. Classic examples are computing the size, the height, or the sum of values of the tree.\n![Tree traversals](/blog/iterative-tree-traversals/traversals.svg)\nBecause rooted trees are recursive data structures, algorithms on trees are most naturally expressed recursively. Here are the three traversals. I use the function `process(node)` as a placeholder for whatever computation the problem calls for.\n```\nvoid preorderTraversal(TreeNode\\* root) {\nif (!root) return;\nprocess(root);\npreorderTraversal(root->left);\npreorderTraversal(root->right);\n}\nvoid inorderTraversal(TreeNode\\* root) {\nif (!root) return;\ninorderTraversal(root->left);\nprocess(root);\ninorderTraversal(root->right);\n}\nvoid postorderTraversal(TreeNode\\* root) {\nif (!root) return;\npostorderTraversal(root->left);\npostorderTraversal(root->right);\nprocess(root);\n}\n```\nSide-note: in C++, pointers are implicitly converted to booleans: a pointer evaluates to true if and only if it is not null. So, in the code above, \"`if (!root)`\" is equivalent to \"`if (root == NULL)`\".\n### Traversal problems on leetcode\n\n- https://leetcode.com/problems/binary-tree-preorder-traversal/\n\n- https://leetcode.com/problems/binary-tree-inorder-traversal/\n\n- https://leetcode.com/problems/binary-tree-postorder-traversal/\n## Why / When to Use an Iterative Traversal\n\nIf the recursive implementation is so simple, why bother with an iterative one? Of course, to avoid stack overflow. Most runtime engines/compilers set a limit on how many nested calls a program can make. For example, according to this article:\n> \\*Default stack size varies between 320k and 1024k depending on the version of Java and the system used. For a 64 bits Java 8 program with minimal stack usage, the maximum number of nested method calls is about 7000.\\*\nIf the height of the tree is larger than this limit, the program will crash with a \\*\\*stack overflow error\\*\\*. A recursive implementation is safe to use if:\n\n- Somehow we know that the input trees will be small enough.\n\n- The tree is \\*balanced\\*, which means that, for each node, the left and right subtrees have roughly the same height. In a balanced tree, the height is guaranteed to be \\*logarithmic\\* on the number of nodes (indeed, that is why balanced BSTs guarantee \\*O(log n)\\* search time), so any tree that fits in RAM (or even disk) will require a tiny number of recursive calls.\nHowever, if we are not in either of the cases above, an iterative solution is safer.\nRecursive and iterative traversals have the same runtime complexity, so this is not a concern when choosing either (all the problems shown in this article can be solved in linear time using either).\nThe main approach for converting recursive implementations to iterative ones is to \"simulate\" the call stack with an actual stack where we push and pop the nodes explicitly. This works great \"out-of-the-box\" with preorder traversal.\nIncidentally, when implementing tree traversals we need to make an implementation choice about how to handle NULL pointers. We can be eager and filter them out before adding them to the stack, or we can be lazy and detect them once we extract them from the stack. Both are fine—what matters is to be deliberate and consistent about which approach we are using. I prefer the latter as it yields slightly shorter code, so I will use it in all the following examples. For comparison, here is the iterative preorder traversal with both approaches:\n```\n//eager NULL checking\nvoid preorderTraversal(TreeNode\\* root) {\nstack stk;\nif (!root) return;\nstk.push(root);\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top();\nstk.pop();\nprocess(node);\nif (node->right) stk.push(node->right);\nif (node->left) stk.push(node->left);\n}\n}\n//lazy NULL checking\nvoid preorderTraversal(TreeNode\\* root) {\nstack stk;\nstk.push(root);\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top();\nstk.pop();\nif (!node) continue;\nprocess(node);\nstk.push(node->right);\nstk.push(node->left);\n}\n}\n```\nNote that \\*\\*the right child is pushed to the stack before the left one\\*\\*. This is because we want the left child to be above in the stack so that it is processed first.\n### Preorder traversal practice problems\n\n- https://leetcode.com/problems/invert-binary-tree/\n\n- https://leetcode.com/problems/maximum-depth-of-binary-tree/\nThis problem asks to find the depth of a binary tree (follow the link for the description and examples). It requires passing information from each node to its children. We can do this by changing the stack to `stack>`, so that we can pass an `int` to each child, as in the solution below:\n```\nint maxDepth(TreeNode\\* root) {\nint res = 0;\nstack> stk;\nstk.push({root, 1}); //node, depth\nwhile (!stk.empty()) {\nauto node = stk.top().first;\nint depth = stk.top().second;\nstk.pop();\nif (!node) continue;\nres = max(res, depth);\nstk.push({node->left, depth+1});\nstk.push({node->right, depth+1});\n}\nreturn res;\n}\n```\nIn the code above, the `{}` notation is used to create pairs (e.g., `{root, 0}`). If one is not familiar with pairs in C++, or is using a language without the equivalent, a simple alternative is to use two separate stacks, one for the nodes and one for the info.\nThe next two problems are similar:\n\n- https://leetcode.com/problems/minimum-depth-of-binary-tree/\n\n- https://leetcode.com/problems/path-sum/\n\n- https://leetcode.com/problems/symmetric-tree/\nA solution for the last one, this time using a stack with a pair of nodes:\n```\nbool isSymmetric(TreeNode\\* root) {\nif (!root) return true;\nstack> stk;\nstk.push({root->left, root->right});\nwhile (!stk.empty()) {\nTreeNode\\* l = stk.top().first;\nTreeNode\\* r = stk.top().second;\nstk.pop();\nif (!l and !r) continue;\nif (!l or !r or l->val != r->val) return false;\nstk.push({l->left, r->right});\nstk.push({l->right, r->left});\n}\nreturn true;\n}\n```\n## Iterative Postorder and Inorder Traversal\n\nWhile iterative preorder traversal is straightforward, with postorder and inorder we run into a complication: we cannot simply swap the order of the lines as with the recursive implementation. In other words, the following does \\*not\\* yield a postorder traversal:\n```\n...\nstk.push(node->right);\nstk.push(node->left);\nprocess(node);\n...\n```\nThe node is still processed before its children, which is not what we want.\n\\*\\*The workaround, once again emulating the recursive implementation, is to visit each node twice.\\*\\* We consider postorder traversal first. In the first visit, we only push the children onto the stack. In the second visit, we do the actual processing.\nThe simplest way to do this is to enhance the stack with a \\*\\*\"visit number flag\"\\*\\*. Implementation-wise, we change the stack to `stack>` so that we can pass the flag along with each node. The iterative postorder looks like this:\n```\nvoid postorderTraversal(TreeNode\\* root) {\nstack> stk; //node, visit #\nstk.push({root, 0});\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->right, 0});\nstk.push({node->left, 0});\n} else { //visit == 1\nprocess(node);\n}\n}\n}\n```\nNote the order in which the nodes are added to the stack when `visit == 0`. The parent ends up under its children, with the left child on top. Since it is the first time that the children are added to the stack, their visit-number flag is 0. For the parent, it is 1.\nFor simplicity, I also follow the convention to always immediately call pop after extracting the top element from the stack.\nThe same approach also works for inorder traversal (that's the point). Here is a version where we visit each node three times: one to push the left child, one to process the node, and one to push the right child.\n```\n//3-visit version\nvoid inorderTraversal(TreeNode\\* root) {\nstack> stk;\nstk.push({root, 0});\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->left, 0});\n} else if (visit == 1) {\nstk.push({node, 2});\nprocess(node);\n} else { //visit == 2\nstk.push({node->right, 0});\n}\n}\n}\n```\nIn fact, the second and third visits can be merged together: processing the node does not modify the stack, so the two visits are followed one after the other anyway. Here is my preferred version:\n```\n//2-visit version\nvoid inorderTraversal(TreeNode\\* root) {\nstack> stk;\nstk.push({root, 0});\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->left, 0});\n} else { //visit == 1\nprocess(node);\nstk.push({node->right, 0});\n}\n}\n}\n```\nFor completeness, here is the version found in most of my top Google hits (see this for a nice explanation):\n```\nvoid inorderTraversal(TreeNode\\* root) {\nstack stk;\nTreeNode\\* curr = root;\nwhile (curr or !stk.empty()) {\nwhile (curr) {\nstk.push(curr);\ncurr = curr->left;\n}\ncurr = stk.top();\nstk.pop();\nprocess(curr);\ncurr = curr->right;\n}\n}\n```\nWhile it is shorter, it cannot be easily converted to postorder traversal, so it is not as flexible. Also, I find it easier to follow the execution flow with the visit-number flag.\n### Inorder traversal practice problems\n\n- https://leetcode.com/problems/kth-smallest-element-in-a-bst/\nA solution (follow the link for the statement and examples):\n```\nint kthSmallest(TreeNode\\* root, int k) {\nint count = 1;\nstack> stk;\nstk.push({root, 0});\nwhile (!stk.empty()) {\nauto node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->left, 0});\n} else { //visit == 1\nif (count == k) return node->val;\ncount++;\nstk.push({node->right, 0});\n}\n}\nreturn -1;\n}\n```\n\n- https://leetcode.com/problems/validate-binary-search-tree/\nA solution:\n```\nbool isValidBST(TreeNode\\* root) {\nint lastVal;\nbool init = false;\nstack> stk;\nstk.push({root, 0});\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->left, 0});\n} else { //second visit\nif (!init) {\ninit = true;\nlastVal = node->val;\n} else {\nif (node->val <= lastVal) return false;\nlastVal = node->val;\n}\nstk.push({node->right, 0});\n}\n}\nreturn true;\n}\n```\n### Postorder traversal practice problems\n\n- https://leetcode.com/problems/balanced-binary-tree/\nThis problem asks to check if a binary tree is balanced. It requires passing information back from the children to the parent node in a postorder traversal. Passing information from the children to the parent is easy with recursion. It can be done both with return values or with parameters passed by reference. For this problem we need to pass two things: a `bool` indicating if the subtree is balanced, and an `int` indicating its height. I use a reference parameter for the latter (returning a `pair` would be cleaner).\n```\nbool isBalancedRec(TreeNode\\* root, int& height) {\nif (!root) {\nheight = 0;\nreturn true;\n}\nint lHeight, rHeight;\nbool lBal = isBalancedRec(root->left, lHeight);\nbool rBal = isBalancedRec(root->right, rHeight);\nheight = max(lHeight, rHeight) + 1;\nreturn lBal && rBal && abs(lHeight - rHeight) <= 1;\n}\nbool isBalanced(TreeNode\\* root) {\nint height;\nreturn isBalancedRec(root, height);\n}\n```\nPassing information from the children to the parent in an iterative implementation is more intricate. There are three general approaches:\n1. Use a hash table mapping each node to the information.\nThis is the easiest way, but also the most expensive.\nWhile the asymptotic runtime is still linear, hash tables generally have significant constant factors.\n```\nbool isBalanced(TreeNode\\* root) {\nstack> stk;\nstk.push({root, 0});\nunordered\\_map height;\nheight[NULL] = 0;\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->right, 0});\nstk.push({node->left, 0});\n} else { // visit == 1\nint lHeight = height[node->left], rHeight = height[node->right];\nif (abs(lHeight - rHeight) > 1) return false;\nheight[node] = max(lHeight, rHeight) + 1;\n}\n}\nreturn true;\n}\n```\n2. Add a field to the definition of the node structure for the information needed.\nThen, we can read it from the parent node by traversing the children's pointers.\nIn Leetcode we cannot modify the `TreeNode` data structure so, to illustrate this approach, I build a new tree first with a new struct:\n```\nstruct MyNode {\nint val;\nint height;\nMyNode \\*left;\nMyNode \\*right;\nMyNode(TreeNode\\* node): val(node->val), height(-1), left(NULL), right(NULL) {\nif (node->left) left = new MyNode(node->left);\nif (node->right) right = new MyNode(node->right);\n}\n};\nbool isBalanced(TreeNode\\* root) {\nif (!root) return true;\nMyNode\\* myRoot = new MyNode(root);\nstack> stk;\nstk.push({myRoot, 0});\nwhile (!stk.empty()) {\nMyNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->right, 0});\nstk.push({node->left, 0});\n} else { // visit == 1\nint lHeight = 0, rHeight = 0;\nif (node->left) lHeight = node->left->height;\nif (node->right) rHeight = node->right->height;\nif (abs(lHeight - rHeight) > 1) return false;\nnode->height = max(lHeight, rHeight) + 1;\n}\n}\nreturn true;\n}\n```\n3. Pass the information through an additional stack.\nThis is the most efficient, but one must be careful to keep both stacks in synch. When processing a node, that node first pops the information from its children, and then pushes its own info for its parent. Here is a solution (with eager NULL-pointer detection):\n```\nbool isBalanced(TreeNode\\* root) {\nif (!root) return true;\nstack> stk;\nstk.push({root, 0});\nstack heights;\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (visit == 0) {\nstk.push({node, 1});\nif (node->right) stk.push({node->right, 0});\nif (node->left) stk.push({node->left, 0});\n} else { // visit == 1\nint rHeight = 0, lHeight = 0;\nif (node->right) {\nrHeight = heights.top();\nheights.pop();\n}\nif (node->left) {\nlHeight = heights.top();\nheights.pop();\n}\nif (abs(lHeight - rHeight) > 1) return false;\nheights.push(max(lHeight, rHeight) + 1);\n}\n}\nreturn true;\n}\n```\n\n- https://leetcode.com/problems/diameter-of-binary-tree/\nThis problem also requires passing information from the children to the parent in a postorder traversal. Here is a solution using the third approach again, but this time with lazy NULL-pointer detection. Note that we push a 0 to the `depths` stack when we extract a NULL pointer from the main stack, and during processing we always do two pops regardless of the number of non-NULL children:\n```\nint diameterOfBinaryTree(TreeNode\\* root) {\nstack> stk;\nstk.push({root, 0});\nstack depths;\nint res = 0;\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) {\ndepths.push(0);\ncontinue;\n}\nif (visit == 0) {\nstk.push({node, 1});\nstk.push({node->right, 0});\nstk.push({node->left, 0});\n} else { //visit == 1\nint rDepth = depths.top();\ndepths.pop();\nint lDepth = depths.top();\ndepths.pop();\nint depth = max(lDepth, rDepth) + 1;\ndepths.push(depth);\nint dia = lDepth + rDepth;\nres = max(res, dia);\n}\n}\nreturn res;\n}\n```\n\n- https://leetcode.com/problems/binary-tree-tilt/\n\n- https://leetcode.com/problems/most-frequent-subtree-sum/\n\n- https://leetcode.com/problems/maximum-product-of-splitted-binary-tree/\n## Traversals in n-ary Trees\n\nSo far, we have looked at binary trees. In an n-ary tree, each node has an arbitrary number of children.\n```\nstruct Node {\nint val;\nvector children;\nNode(int val): val(val), children(0) {}\n};\n```\nFor n-ary trees, preorder traversal is also straightforward, and inorder traversal is not defined.\nFor postorder traversal, we can use a visit-number flag again. Two visits suffice for each node: one to push all the children into the stack, and another to process the node itself. I do not include the code here because it is very similar to the binary tree case.\nConsider a more complicated setting where we need to compute something at the node after visiting each child. Let's call this \"interleaved traversal\". I use `process(node, i)` as placeholder for the computation done before visiting the i-th child. Here is the recursive implementation and the corresponding iterative one using visit-number flags.\n```\n//recursive\nvoid interleavedTraversal(Node\\* root) {\nif (!root) return;\nint n = root->children.size();\nfor (int i = 0; i < n; i++) {\nprocess(root, i);\ninterleavedTraversal(root->children[i]);\n}\n}\n//iterative\nvoid interleavedTraversal(Node\\* root) {\nstack> stk;\nstk.push({root, 0});\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top().first;\nint visit = stk.top().second;\nstk.pop();\nif (!node) continue;\nint n = node->children.size();\nif (visit < n) {\nstk.push({node, visit+1});\nprocess(node, visit);\nstk.push({node->children[visit], 0});\n}\n}\n}\n```\n### N-ary tree practice problems\n\n- https://leetcode.com/problems/n-ary-tree-preorder-traversal/\n\n- https://leetcode.com/problems/n-ary-tree-postorder-traversal/\n## An Alternative Way of Passing the Visit Flag\n\nThe common framework to all our solutions has been to pass a visit-number flag along with the nodes on the stack. User \"heiswyd\" on leetcode posted here an alternative way to pass the flag implicitly: initially, it pushes each node on the stack twice. Then, it can distinguish between the first visit and the second visit by checking whether the node that has just been extracted from the stack matches the node on top of the stack. This happens only when we extract the first of the two occurrences. Post-order traversal looks like this:\n```\nvoid postorderTraversal(TreeNode\\* root) {\nstack stk;\nstk.push(root);\nstk.push(root);\nwhile (!stk.empty()) {\nTreeNode\\* node = stk.top();\nstk.pop();\nif (!node) continue;\nif (!stk.empty() and stk.top() == node) {\nstk.push(node->right);\nstk.push(node->right);\nstk.push(node->left);\nstk.push(node->left);\n} else {\nprocess(node);\n}\n}\n}\n```\nIt is cool, but I prefer passing the flag explicitly for clarity.",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/iterativetreetraversal?category=dsa"
    }
  ]
}