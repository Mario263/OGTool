{
  "site": "https://nilmamano.com/blog/category/dsa",
  "items": [
    {
      "title": "BCtCI Free Resources",
      "content": "A comprehensive list of all free resources available from Beyond Cracking the Coding Interview, including chapters, tools, templates, and guides.",
      "content_type": "podcast_transcript",
      "source_url": "https://nilmamano.com/blog/bctci-free-resources?category=dsa"
    },
    {
      "title": "BCtCI Chapter: Set & Map Implementations",
      "content": "# BCtCI Chapter: Set & Map Implementations\n\nAugust 5, 2025\n![BCtCI Chapter: Set & Map Implementations](/blog/set-and-map-implementations/fig3.png)\nThis is an online chapter from Beyond Cracking the Coding\nInterview, co-authored with Gayle Laakmann McDowell, Mike Mroczka, and Aline Lerner.\nYou can try all the problems from this chapter with our AI interviewer at\nbctci.co/set-and-map-implementations.\nYou can also find full solutions in \\*\\*Python, C++, Java, and JS\\*\\* there.\nThis is a Tier 3 DS&A topic. The book contains all the Tier 1 & 2 topics. You can find other Tier 3 topics online at bctci.co (Part VIII of the ToC).\nReport bugs or typos at bctci.co/errata.\n\\*\\*Prerequisites:\\*\\* Dynamic Arrays, Sets & Maps. It is recommended to review those chapters from BCtCI before reading this one.\n# Introduction\n\nWe saw how to \\*use\\* sets and maps in the 'Sets & Maps' chapter (bctci.co/sets-and-maps). We saw that they often come in handy in coding interviews, and we discussed how to use them effectively.\nSets and maps are implemented in two main ways:\n\n- Using hash tables.\n\n- Using self-balancing binary search trees.\nWe can distinguish between the implementations by calling one \\*HashSet\\* and \\*HashMap\\* and the other \\*TreeSet\\* and \\*TreeMap\\*. They have different use cases in advanced scenarios, but for the basic APIs we'll define, the hash-table-based implementations are more common, as they have better asymptotic runtimes and are faster in practice.\nMost programming languages provide built-in hash sets and hash maps. In this chapter, we will look at how they are implemented. This will allow us to learn about the important concepts of hashing and hash tables. See the 'Self-Balancing BSTs' section in the Trees chapter for a discussion on tree-based implementations.\nWe'll start with sets, which are a bit simpler than maps.\n# Hash Set Implementation\n\nThe set API is simple:\n```\nadd(x): if x is not in the set, adds x to the set\nremove(x): if x is in the set, removes it from the set\ncontains(x): returns whether the element x is in the set\nsize(): returns the number of elements in the set\n```\nA question for the reader: Why do sets not have a function like `set(i, x)`, which sets an element at a particular index?1\nWe will build upon the concepts of dynamic arrays from the 'Dynamic Arrays' chapter (see bctci.co/implement-dynamic-array) to show how to implement sets very efficiently, which is what makes sets a versatile data structure for coding interviews.\n## Naive Set Implementation\n\nLet's start with a naive implementation idea and then identify its weaknesses so we can improve it—like we often do in interviews. What if we simply store all the set elements in a dynamic array? The issue with this idea is that checking if an element is in the set would require a linear scan, so the `contains()` method would not be very efficient.\nBy comparison, dynamic arrays do lookups \\*by index\\*, which means that we can jump directly to the correct memory address based on the given index. We would like to be able to do something similar for sets, where, just from an element `x`, we can easily find where we put it in the dynamic array.\nHere is a promising idea in this direction, though still unpolished. Say you have a set of integers. Instead of putting all the elements in a single dynamic array, initialize your set data structure with, say, `1000` dynamic arrays, and then store each integer `x` in the array at index `x % 1000`. In this way, we no longer need to scan through a list of every element each time: if the user calls `contains(1992)`, we only need to check the array at index `1992 % 1000 = 992`. That is, we only need to scan through integers that have the same remainder when divided by `1000`.\nThis crude idea could work well in certain situations, but it has two critical shortcomings that we still have to address.\n\n- The first shortcoming has to do with the \\*\\*distribution\\*\\* of the integers in the set. Consider a situation where someone uses a set to store file sizes in bytes, and in their particular application all the file sizes happened to be in whole kilobytes. Every element stored in the set, like `1000`, `2000`, or `8000`, would end up in the array at index `0`. If all the elements end up in the same index, we are back at the starting point, having to do linear scans through all the elements.\n\n- The second shortcoming is that even if the elements were perfectly distributed among the `1000` arrays, fixing the number of arrays to `1000` is too rigid. It might be overkill if we have only a few elements, and it might not be enough if we store a really large number of them.\nIn the next section, we will use hash functions to address the first shortcoming. For the second shortcoming, we will see a familiar technique from dynamic arrays: dynamic resizing.\n## Hash Functions\n\nA \\*hash function\\* is a function that maps inputs of some type, such as numbers or strings, into a \"random-looking\" number in a predetermined output range. For instance, if `h(x)` is a hash function for integers with an output range of `0` to `999`, then it could map `1` to `111`, `2` to `832`, and `3` to itself. Even though the mapping seems \"random,\" it is important that each input always maps to the same value. That is, `h(1)` must always be `111`. We say that `111` is the \\*hash\\* of `1`.\nIn an actual hash function implementation, hashes are not truly random, but we want them to be \"random-looking\" because \\*\\*we consider a hash function \\*good\\* if it spreads the input values evenly across the output range, even if the distribution of input values is skewed in some way or follows some non-random pattern.\\*\\* For instance, `h(x) = x % 1000` is \\*not\\* a good hash function because, even though each number from `0` to `999` is the hash of the same fraction of input values, we saw that we could get a very uneven distribution for certain input patterns—like if every input was a multiple of `1000`. The challenge, then, is to spread the inputs evenly for \\*any\\* input pattern.\n### A simple hash function for integers\n\nThere are many interesting ways to hash integers into a fixed range such as `[0, 999]`. In this section, we will show a simple one known as \\*multiplicative hashing\\*. It may not have the best distribution properties, but it suffices to understand the concept and it runs in constant time.\nWe'll use `m` to denote the upper bound of the output range, which is excluded. In our example, `m = 1000`. Our hash function is based on the following observation: if we multiply an input integer by a constant between `0` and `1` with \"random-looking\" decimals such as `0.6180339887`, we get a number with a \"random-looking\" fractional part. For instance,\n\n- `1 \\* 0.618... = 0.618...`\n\n- `2 \\* 0.618... = 1.236...`\n\n- `3 \\* 0.618... = 1.854...`\nWe can then take the fractional part alone, which is between `0` and `1`, and multiply it by `m` to scale it up to the range between `0` and `m`. Finally, we can truncate the decimal part and output the integer part of this number, which will be a \"random-looking\" number between `0` and `m-1`.2\n![The steps of multiplicative hashing for an output range of [0, 999], using 0.6180339887 as the constant C](/blog/set-and-map-implementations/fig1.png)\n\\*\\*Figure 1.\\*\\* The steps of multiplicative hashing for an output range of [0, 999], using 0.6180339887 as the constant C.\n```\nC = 0.6180339887 # A \"random-looking\" fraction between 0 and 1.\n\ndef h(x, m):\nx \\*= C # A product with \"random-looking\" decimals.\n\nx -= int(x) # Keep only the fractional part.\n\nx \\*= m # Scale the number in [0, 1) up to [0, m).\n\nreturn int(x) # Return the integer part.\n\n```\nWhen `m = 1000`, the function amounts to returning the first `3` decimals of `x \\* C` as our output hash between `000` and `999`. For example:\n\n- `h(1, 1000) = 618` because `1 \\* 0.618... = 0.618...`\n\n- `h(2, 1000) = 236` because `2 \\* 0.618... = 1.236...`\n\n- `h(1000, 1000) = 33` because `1000 \\* 0.618... = 618.033...`\nThere will be cases where different inputs have the same hash. Those are called \\*collisions\\*. For instance, `2` collides with `612`:\n\n- `h(612, 1000) = 236` because `612 \\* 0.618... = 378.236...`\n\\*\\*If the number of possible inputs is larger than the output range, collisions are unavoidable.\\*\\*3\n### Hash functions in sets\n\nIf you think about it, in our naive set implementation, we used a sort of hash function to map inputs to one of our `1000` dynamic arrays: `h(x, 1000) = x % 1000`. However, as we saw, this hash function could result in a ton of collisions for certain input patterns. If we had used multiplicative hashing instead, we would have distributed the inputs much better. We'll see how it all comes together at the end of the section.\n### Hashing strings\n\nWe want to be able to hash different types, not just integers. Strings are a type we often need to hash, but this poses the extra challenge of different input and output types: string `->` number.\nWe'll follow a two-step approach:\n1. Convert the string into a number,\n2. Once we have a number, use the modulo operator (`%`) to get a remainder that fits in the output range. For instance, if the output range is `0` to `999`, we would set `m = 1000` and do:\n```\ndef h(str, m):\nnum = str\\_to\\_num(str)\nreturn num % m\n```\nIn Step (2), we \\*could\\* apply integer hashing (like multiplicative hashing) to the number returned in Step (1), but if the numbers returned by Step (1) are unique enough, the modulo operation is usually enough.\nSo, how can we convert strings into numbers? Since characters are encoded in binary, we can treat each character as a number. For instance, in Python, `ord(c)` returns the numerical code of character `c`. For this section, we will assume that we are dealing with ASCII strings, where the most common characters, like letters, numbers, and symbols, are encoded as 7-bit strings (numbers from `0` to `127`).4\nCan we use ASCII codes of the individual characters to convert a whole string into a single number?\n\\*\\*Problem set.\\*\\* Each of the following proposed implementations for the `str\\_to\\_num()` method, which converts strings to integers, is not ideal. Do you see why? Can you identify which input strings will result in \\*collisions\\* (the same output number)?\n1. Return the ASCII code of the first character in the string.\n2. Return the ASCII code of the first character in the string multiplied by the length of the string.\n3. Return the sum of the ASCII code of each character.\n4. Return the sum of the ASCII code of each character multiplied by the character's index in the string (starting from `1`).\n\\*\\*Solution.\\*\\*\n1. This function is bad because any two strings starting with the same character, like `\"Mroczka\"` and `\"McDowell\"`, end up with the same number: `ASCII(M) = 77`. Another way to see that it is bad is that it can only output `128` possible different values, which is too small to avoid collisions.\n2. This function is less bad because, at least, it can output a much larger set of numbers. However, there can still be many collisions. Any two strings starting with the same character and with the same length get the same number, like `\"aa\"` and `\"az\"`: `97 \\* 2 = 194`.\n3. This function is much better because it factors in information from all the characters. Its weakness is that the \\*order\\* of the characters does not matter: the string \"spot\" maps to the number `115 + 112 + 111 + 116 = 454`, but so do other words like `\"stop\"`, `\"pots\"`, and `\"tops\"`.\n4. This function is even better and results in fewer collisions because it uses information from all the characters \\*and\\* also uses the order. However, it is still possible to find collisions, like `\"!!\"` and `\"c\"`. The ASCII code for `'!'` is `33`, so the number for `\"!!\"` is `33 \\* 1 + 33 \\* 2 = 99`, and the ASCII code for `'c'` is also `99`.\nIt turns out that \\*\\*there is a way to map ASCII strings to numbers without ever having any collisions\\*\\*. The key is to think of the whole string as a number where each character is a 'digit' in base `128` (see Figure 2). For instance, `\"a\"` corresponds to the number `97` because that's the ASCII code for `'a'`, and `\"aa\"` corresponds to `12513` because `(97 \\* 128^1) + (97 \\* 128^0) = 12513`.\n![Visualizing a decimal number like 5213 as a sum of consecutive powers of 10, and an ASCII string like 'spot' as a sum of consecutive powers of 128](/blog/set-and-map-implementations/fig2.png)\n\\*\\*Figure 2.\\*\\* We can visualize a decimal number like 5213 as a sum of consecutive powers of 10. Likewise, we can visualize an ASCII string like 'spot' as a sum of consecutive powers of 128. This produces a unique numeric value for every ASCII string.\n```\ndef str\\_to\\_num(str):\nres = 0\nfor c in str:\nres = res \\* 128 + ord(c)\nreturn res\n```\nThe cost to pay for having unique numbers is that we quickly reach huge numbers, as shown in Figure 2. In fact, the numbers could easily result in overflow when using 32-bit or 64-bit integers (`128^10 > 2^64`, so any string of length `11` or more would \\*definitely\\* overflow with 64-bit integers).\nTo prevent this, we add a modulo operation to keep numbers in a manageable range. For instance, if we want the number to fit within a 32-bit integer, we can divide our number by a \\*\\*large prime\\*\\* that fits in `32` bits like `10^9 + 7` and keep only the remainder.5\n```\nLARGE\\_PRIME = 10\\*\\*9 + 7\ndef str\\_to\\_num(str):\nres = 0\nfor c in str:\nres = (res \\* 128 + ord(c)) % LARGE\\_PRIME\nreturn res\n```\nWe want `LARGE\\_PRIME` to be large to maximize the range of possible remainders and have fewer collisions. We want it to be prime because non-prime numbers can lead to more collisions for specific input distributions. For instance, when dividing numbers by `12`, which has `3` as a factor, every multiple of `3` has a remainder of `0`, `3`, `6`, or `9`. That means that if our input distribution consisted of only multiples of `3`, we would have a lot of collisions. Prime numbers like `11` do not have this issue except for multiples of `11` itself.6\nThis is a good way to map strings into integers without overflowing. The runtime is proportional to the string's length. In the 'Rolling Hash Algorithm' section, we will see another application of this method.\nAside\nHash functions have many applications beyond hash sets and maps. For instance,\nthey are used to guarantee file integrity when sending files over the network.\nThe sender computes a 64-bit hash of the entire file and sends it along with\nthe file. The recipient can then compute the hash of the received file and\ncompare it with the received hash. If the two hashes match, it means that, in\nall likelihood, the file was not corrupted on the way (with a good hash\nfunction and an output range of `2^64` different hashes, it would be a freak\naccident for both the original file and the corrupted file to have the same\nhash!)\n## Putting it All Together\n#### Problem 1: Hash Set Class\n\nTry it yourself →\nImplement a hash set which supports `add()`, `contains()`, `remove()`, and `size()`.\n#### Solution 1: Hash Set Class\n\nFull code & other languages →\nOur final set implementation will consist of a dynamic array of \\*buckets\\*, each bucket being itself a dynamic array of elements.7 The outer array is called a \\*hash table\\*. The number of elements is called the \\*size\\*, and the number of buckets is called the \\*capacity\\*. We can start with a modest capacity, like `10`:\n```\nclass HashSet:\ndef \\_\\_init\\_\\_(self):\nself.capacity = 10\nself.\\_size = 0\nself.buckets = [[] for \\_ in range(self.capacity)]\ndef size(self):\nreturn self.\\_size\n```\nIn this context, size has the same meaning as for dynamic arrays, but capacity is used a bit differently, since all buckets can be used from the beginning. We use a hash function `h()` to determine in which bucket to put elements: element `x` goes in the bucket at index `h(x, m)`, where `m` is the number of buckets in the hash table (the capacity).8 The specific hash function will depend on the type of the elements, but using a good hash function will guarantee that the buckets' lengths are well-balanced.\n```\ndef contains(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn True\nreturn False\n```\nThe final piece to complete our set data structure involves how to dynamically adjust the number of buckets based on the number of elements in the set. On the one hand, we do not want to waste space by having a lot of empty buckets. On the other hand, \\*\\*in order for lookups to take constant time, each bucket should contain at most a constant number of elements.\\*\\* The `size/capacity` ratio is called the \\*load factor\\*. In other words, the load factor is the average length of the buckets.\nA good load factor to aim for is `1`. That means that, on average, each bucket contains one element. Of course, some buckets will be longer than others, depending on how lucky we get with collisions, which are unavoidable. But, if the hash function manages to map elements to buckets approximately uniformly, with a load factor of `1` the \\*\\*expected\\*\\* longest bucket length is still constant.\nRemember how, with dynamic arrays, we had to resize the fixed-size array whenever we reached full capacity? With hash tables, we will do the same: we will relocate all the elements to a hash table twice as big whenever we reach a load factor of `1`.9 The only complication is that when we move the elements to the new hash table, we will have to re-hash them because the output range of the hash function will have also changed. We now have all the pieces for a full set implementation.\n```\ndef add(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn\nself.buckets[hash].append(x)\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\ndef resize(self, new\\_capacity):\nnew\\_buckets = [[] for \\_ in range(new\\_capacity)]\nfor bucket in self.buckets:\nfor elem in bucket:\nhash = h(elem, new\\_capacity)\nnew\\_buckets[hash].append(elem)\nself.buckets = new\\_buckets\nself.capacity = new\\_capacity\n```\nFinally, when removing elements from the set, we downsize the number of buckets if the load factor becomes too small, similar to how we did for dynamic arrays. We can halve the number if the load factor drops below 25%.\n```\ndef remove(self, x):\nhash = h(x, self.capacity)\nfor i, elem in enumerate(self.buckets[hash]):\nif elem == x:\nself.buckets[hash].pop(i)\nself.\\_size -= 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor < 0.25 and self.capacity > 10:\nself.resize(self.capacity // 2)\n```\nThis completes our set implementation.\n![Evolution of a set of integers after a sequence of additions and removals using multiplicative hashing](/blog/set-and-map-implementations/fig3.png)\n\\*\\*Figure 3.\\*\\* Evolution of a set of integers after a sequence of additions and removals. We are using multiplicative hashing as we described above. Integers 24 and 3 both have hash 8, so they get stored sequentially in the same bucket.\n![Resize of the hash table with a load factor of 6/5 > 1, showing how elements need to be rehashed](/blog/set-and-map-implementations/fig4.png)\n\\*\\*Figure 4.\\*\\* Resize of the hash table with a load factor of 6/5 > 1. We need to rehash every element. (This figure shows 5 buckets for illustration purposes, but our implementation starts with 10.)\n## Hash Set Analysis\n\nThe `contains()` method has to compute the hash of the input and then loop through the elements hashed to a particular bucket. Assuming we have a decent hash function and a reasonable load factor, we expect each slot to be empty or contain very few elements. Thus, the \\*\\*expected runtime\\*\\* for a set of integers is `O(1)`. It is worth mentioning that this expectation only applies with a high-quality hash function!\nThe `add()` method has the same amortized analysis as our dynamic array. Most additions take `O(1)` expected time, since we just find the correct bucket and append the value to it. We \\*occasionally\\* need to resize the array, which takes linear time, but, like with dynamic arrays, this happens so infrequently that the \\*\\*amortized runtime\\*\\* per addition over a sequence of additions is still `O(1)`. The analysis for `remove()` is similar.\nThus, we can expect \\*\\*all the set operations to take constant time or amortized constant time\\*\\*. If the elements are strings, we also need to factor in that hashing a string takes time proportional to the size of the string. One simple way to phrase this is that we can expect the operations to take `O(k)` amortized time, where `k` is the maximum length of any string added to the set.\n## Hash Set Problem Set\n\nThe following problem set asks \\*data structure design\\* questions related to hash sets.\n#### Problem 2: Hash Set Class Extensions\n\nTry it yourself →\nAdd the following methods to our `HashSet` class. For each method, provide the time and space analysis.\n1. Implement an `elements()` method that takes no input and returns all the elements in the set in a dynamic array.\n2. Implement a `union(s)` method that takes another set, `s`, as input and returns a new set that includes all the elements in \\*either\\* set. Do not modify either set.\n3. Implement an `intersection(s)` method that takes another set as input, `s`, and returns a new set that includes only the elements contained in \\*both\\* sets. If the sets have no elements in common, return an empty set. Do not modify either set.\n#### Problem 3: Multiset\n\nTry it yourself →\nA \\*multiset\\* is a set that allows multiple copies of the same element. Implement a `Multiset` class with the following methods:\n```\nadd(x): adds a copy of x to the multiset\nremove(x): removes a copy of x from the multiset\ncontains(x): returns whether x is in the multiset (at least one copy)\nsize(): returns the number of elements in the multiset (including copies)\n```\n\\*\\*Problem set solutions.\\*\\*\n#### Solution 2: Hash Set Class Extensions\n\nFull code & other languages →\n1. `elements()`: We can start by initializing an empty dynamic array. Then, we can iterate through the list of buckets, and append to the list all the elements in each bucket. The runtime is `O(capacity + size) = O(size)` (since the load factor is at least 25%, so the capacity is at most four times the size), and the space complexity is `O(size)`.\n2. `union(s)`: We can start by initializing an empty set. Then, we can iterate through the elements in both `self` and `s` and call `add(x)` on the new set for each element `x`.\n3. `intersection(s)`: We can start by initializing an empty set. Then, we can iterate through the elements in `self` and call `add(x)` on the new set only for those elements where `s.contains(x)` is true.\n#### Solution 3: Multiset\n\nFull code & other languages →\nWe can approach it in two ways:\n1. We can tweak the `HashSet` implementation we already discussed. The `contains()`, `remove()`, and `size()` methods do not change. For `add()`, we need to remove the check for whether the element already exists.\n2. Alternatively, we could implement the multiset as a hash map with the elements in the set as keys and their counts as values.\nApproach 2 is more common because it can save space: if we add `n` strings of length `k`, we'll use only `O(k)` space instead of `O(n \\* k)` for Approach 1. See the next section for more on maps.\n# Hash Map Implementation\n\nAs we have seen, sets allow us to store unique elements and provide constant time lookups (technically, it is amortized constant time, and only with a good hash function). Maps are similar, but the elements in a map are called \\*keys\\*, and each key has an associated \\*value\\*. Like elements in sets, map keys cannot have duplicates, but two keys can have the same value. The type of values can range from basic types like booleans, numbers, or strings to more complex types like lists, sets, or maps.\nMap API:\n```\nadd(k, v): if k is not in the map, adds key k to the map with value v\nif k is already in the map, updates its value to v\nremove(k): if k is in the map, removes it from the map\ncontains(k): returns whether the key k is in the map\nget(k): returns the value for key k\nif k is not in the map, returns a null value\nsize(): returns the number of keys in the map\n```\nFor instance, we could have a map with employee IDs as keys and their roles as values. This would allow us to quickly retrieve an employee's role from their ID.\nThankfully, we do not need to invent a new data structure for maps. Instead, we reuse our `HashSet` implementation and only change what we store in the buckets. Instead of just storing a list of elements, we store a list of key–value pairs. As a result, maps achieve \\*\\*amortized constant time for all the operations\\*\\* like sets.\nWe will leave the details of map internals as our final problem set in this chapter. In particular, you can find the full `HashMap` implementation in the solution to Problem 4 (bctci.co/hash-map-class).\n## Hash Map Problem Set\n\nThe following problem set asks \\*data structure design\\* questions related to maps.\n#### Problem 4: Hash Map Class\n\nTry it yourself →\nImplement a `HashMap` class with the API described in the 'Hash Map Implementation' section.\nHint: you can modify the existing `HashSet` class to store key–value pairs.\n#### Problem 5: Hash Map Class Extensions\n\nTry it yourself →\nProvide the time and space analysis of each method.\n1. Implement a `keys()` method that takes no input and returns a list of all keys in the map. The output order doesn't matter.\n2. Implement a `values()` method that takes no input and returns a list of the values of all the keys in the map. The output order doesn't matter. If a value appears more than once, return it as many times as it occurs.\n#### Problem 6: Multimap\n\nTry it yourself →\nA \\*multimap\\* is a map that allows multiple key–value pairs with the same key. Implement a `Multiset` class with the following methods:\n```\nadd(k, v): adds key k with value v to the multimap, even if key k is already found\nremove(k): removes all key-value pairs with k as the key\ncontains(k): returns whether the multimap contains any key-value pair with k as the key\nget(k): returns all values associated to key k in a list\nif there is none, returns an empty list\nsize(): returns the number of key-value pairs in the multiset\n```\n\\*\\*Problem set solutions.\\*\\*\n#### Solution 4: Hash Map Class\n\nFull code & other languages →\nWe can adapt the `HashSet` class. The main difference is that, in `add()`, we need to consider the case where the key already exists, and we just update its value.\nThis was the `add()` method for the `HashSet` class:\n```\ndef add(self, x):\nhash = h(x, self.capacity)\nfor elem in self.buckets[hash]:\nif elem == x:\nreturn\nself.buckets[hash].append(x)\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\n```\nAnd this is the modified version for the `HashMap` class:\n```\ndef add(self, k, v):\nhash = h(k, self.capacity)\nfor i, (key, \\_) in enumerate(self.buckets[hash]):\nif key == k:\nself.buckets[hash][i] = (k, v)\nreturn\nself.buckets[hash].append((k, v))\nself.\\_size += 1\nload\\_factor = self.\\_size / self.capacity\nif load\\_factor > 1:\nself.resize(self.capacity \\* 2)\n```\nThe `get()` method is very similar to `contains()`.\n#### Solution 5: Hash Map Class Extensions\n\nFull code & other languages →\n1. `keys()`: This is analogous to the `elements()` method from the Set Quiz.\n2. `values()`: This is like `keys()`, but getting the values instead.\n#### Solution 6: Multimap\n\nFull code & other languages →\nA multimap with values of type `T` can be implemented as a map where the values are arrays of elements of type `T`. The `add()` method appends to the array, the `get()` method returns the array, and `remove()` works as usual.\n# Rolling Hash Algorithm\n\nBefore wrapping up the chapter, we will illustrate a clever application of hash functions outside of hash sets and maps. Recall 'Problem 3: String Matching' from the String Manipulation chapter (bctci.co/string-matching):\nImplement an `index\\_of(s, t)` method, which returns the first index where string `t` appears in string `s`, or `-1` if `s` does not contain `t`.\nA naive approach would be to compare `t` against every substring of `s` of the same length as `t`. If `sn` and `tn` are the lengths of `s` and `t`, there are `sn - tn + 1` substrings of `s` of length `tn`. Comparing two strings of length `tn` takes `O(tn)` time, so in total, this would take `O((sn - tn + 1) \\* tn) = O(sn \\* tn)` time.\nWe can do better than this naive algorithm. The Knuth-Morris-Pratt (KMP) algorithm is a famous but tricky algorithm for this problem that runs in `O(sn)` time, which is optimal. Here, we will introduce the \\*rolling hash algorithm\\*, another famous algorithm that is also optimal and which we find easier to learn and replicate in interviews.\nThe basic idea is to compare string hashes instead of strings directly. For instance, if `s` is `\"cbaabaa\"` and `t` is `\"abba\"`, we compare `h(\"abba\")` with `h(\"cbaa\")`, `h(\"baab\")`, `h(\"aaba\")`, and `h(\"abaa\")`. If no hashes match, we can \\*conclusively\\* say that `t` does not appear in `s`. If we have a match, it \\*could\\* be a collision, so we need to verify the match with a regular string comparison. However, with a good hash function, 'false positives' like this are unlikely.\nSince we are not actually mapping strings to buckets like in a hash table, we can use the `str\\_to\\_num()` function that we created in the 'Hashing strings' section as our hash function.\nHowever, computing the hash of each substring of `s` of length `tn` would still take `O(sn \\* tn)` time, so this alone doesn't improve upon the naive solution. The key optimization is to compute a \\*rolling hash\\*: we use the hash of each substring of `s` to compute the hash of the next substring efficiently.\n![The hashes of all the substrings of s = 'cbaabaa' of length 4](/blog/set-and-map-implementations/fig5.png)\n\\*\\*Figure 5.\\*\\* The hashes of all the substrings of length 4 of s = 'cbaabaa'.\nSo, how can we go from, e.g., `h(\"cbaa\")` to `h(\"baab\")`? We can do it in three steps. Recall that `ASCII('a') = 97`, `ASCII('b') = 98`, and `ASCII('c') = 99`. We start with:\n`h(\"cbaa\") = 99 \\* 128^3 + 98 \\* 128^2 + 97 \\* 128^1 + 97 \\* 128^0 = 209236193`\nThen:\n1. First, we remove the contribution from the first letter, `'c'`, by subtracting `99 \\* 128^3`. Now, we have:\n`98 \\* 128^2 + 97 \\* 128^1 + 97 \\* 128^0 = 1618145`\n2. Then, we multiply everything by `128`, which effectively increases the exponent of the `128` in each term by one. This is kind of like \\*shifting\\* every letter to the left in Figure 5. Now, we have:\n`98 \\* 128^3 + 97 \\* 128^2 + 97 \\* 128^1 = 207122560`\n3. Finally, we add the contribution of the new letter, `'b'`, which is `98 \\* 128^0`. We end up with:\n`98 \\* 128^3 + 97 \\* 128^2 + 97 \\* 128^1 + 98 \\* 128^0 = 207122658`\nThis is `h(\"baab\")`. We are done!\nHere is a little function to do these steps. Note that we use modulo (`%`) at each step to always keep the values bounded within a reasonable range.10\n```\n# Assumes that current\\_hash is the hash of s[i:i+tn].\n# Assumes that first\\_power is 128^(tn-1) % LARGE\\_PRIME.\n# Returns the hash of the next substring of s of length tn: s[i+1:i+tn+1].\n\ndef next\\_hash(s, tn, i, current\\_hash, first\\_power):\nres = current\\_hash\n# 1. Remove the contribution from the first character (s[i]).\n\nres = (res - ord(s[i]) \\* first\\_power) % LARGE\\_PRIME\n# 2. Increase every exponent.\n\nres = (res \\* 128) % LARGE\\_PRIME\n# 3. Add the contribution from the new character (s[i+tn]).\n\nreturn (res + ord(s[i + tn])) % LARGE\\_PRIME\n```\nEach step takes constant time, so, from the hash of a substring of `s`, we can compute the hash of the \\*\\*next\\*\\* substring in constant time.\nWe pass `128^(tn-1) % 10^9 + 7` as the `first\\_power` parameter so that we do not need to compute it for each call to `next\\_hash()`, as that would increase the runtime. This way, we compute the hashes of all the substrings of length `t` in `O(sn)` time. Putting all the pieces together, we get the rolling hash algorithm:\n```\nLARGE\\_PRIME = 10\\*\\*9 + 7\ndef power(x):\nres = 1\nfor i in range(x):\nres = (res \\* 128) % LARGE\\_PRIME\nreturn res\ndef index\\_of(s, t):\nsn, tn = len(s), len(t)\nif tn > sn:\nreturn -1\nhash\\_t = str\\_to\\_num(t)\ncurrent\\_hash = str\\_to\\_num(s[:tn])\nif hash\\_t == current\\_hash and t == s[:tn]:\nreturn 0 # Found t at the beginning.\n\nfirst\\_power = power(tn - 1)\nfor i in range(sn - tn):\ncurrent\\_hash = next\\_hash(s, tn, i, current\\_hash, first\\_power)\nif hash\\_t == current\\_hash and t == s[i + 1:i + tn + 1]:\nreturn i+1\nreturn -1\n```\nWith a good hash function like the one we used, which makes collisions extremely unlikely, this algorithm takes `O(sn)` time, which is optimal.\nThis algorithm could also be called the \\*sliding\\* hash algorithm since it is an example of the sliding window technique that we learned about in the Sliding Window chapter (bctci.co/sliding-windows).\n# Key Takeaways\n\nThe following table should make it clear why hash sets and hash maps, as we built them in this chapter, are one of the first data structures to consider when we care about performance and big O analysis:\n| Hash Sets | Time | Hash Maps | Time |\n| --- | --- | --- | --- |\n| `add(x)` | O(1) | `add(k, v)` | O(1) |\n| `remove(x)` | O(1) | `remove(k)` | O(1) |\n| `contains(x)` | O(1) | `contains(k)` | O(1) |\n| `size()` | O(1) | `get(k)` | O(1) |\n| | | `size()` | O(1) |\n\\*\\*Table 1.\\*\\* Set and map operations and worst-case runtimes.\nHowever, it is important to remember the caveats:\n\n- The runtimes for `add()` and `remove()` are \\*\\*amortized\\*\\* because of the dynamic resizing technique, which can make a single `add()` or `remove()` slow from time to time.\n\n- All the runtimes (except `size()`) are \\*\\*expected runtimes\\*\\*, and only when we are using a good hash function. In this chapter, we saw examples of good hash functions for integers and strings, and similar ideas can be used for other types.\n\n- A common mistake is forgetting that hashing a string takes time proportional to the length of the string, so hash set and hash map operations do, in fact, \\*\\*not\\*\\* take constant time for non-constant length strings.\nThe 'Sets & Maps' chapter (bctci.co/sets-and-maps) shows how to make the most of these data structures in the interview setting.\n## Footnotes\n\n1. Unlike arrays, sets do not have index-based setters and getters because elements are not in any particular order. ↩\n2. A bad choice for the constant between `0` and `1` would be a simple fraction like `1/2`, because then all even numbers would get hashed to `0` and all odd numbers would get hashed to `m/2`. Here, we are using the inverse of the golden ratio, `φ = 1.618033...` because it is the constant Donald Knuth used when he introduced this technique in his book The Art of Computer Programming. When using `1/φ` as the constant, the hash function is known as \"Knuth's multiplicate hashing,\" as well as \"Fibonacci hashing\" because of the relationship between the golden ratio and the fibonacci sequence. However, the exact constant used is not the point—you do not need to memorize any \"magic constants\" for your interviews. ↩\n3. Mathematicians call this the \"pigeonhole principle\". If you think of the inputs as pigeons and the hashes as holes, it says that if there are more pigeons than holes, some pigeons (inputs) will have to share a hole (have the same hash). ↩\n4. See the 'String Manipulation' Chapter in BCtCI for more on the ASCII format. ↩\n5. Intermediate values may still exceed `32` bits, so, for typed languages, make sure to use 64-bit integers for the intermediate operations. ↩\n6. There is nothing special about the number `10^9 + 7`—it is just a large prime that fits in a 32-bit integer and is easy to remember because it is close to a power of ten. Any large prime would work well. Again, it is not important to memorize magic constants for interviews—you can always use a placeholder variable \"LARGE\\\\_PRIME\". ↩\n7. Another popular data structure for each bucket is a linked list, which we learned about in the Linked Lists chapter (bctci.co/linked-lists). ↩\n8. This strategy of handling collisions by putting all the colliding elements in a list is called \\*separate chaining\\*. There are other strategies, like \\*linear probing\\* or \\*cuckoo hashing\\*, but knowing one is probably enough. ↩\n9. The exact load factor value is not important. Values slightly less or slightly more than one still have the same properties. Between a load factor of `0.5` and `1.5` there is a bit of a trade off between space and lookup time, but the asymptotic complexities don't change. ↩\n10. Depending on the language, we have to be careful about Step 1 because the numerator (`res - ord(s[i]) \\* first\\_power`) can be negative. In Python, `-7 % 5 = 3`, which is what we want, but in other languages like JS, Java, and C++, `-7 % 5 = -2`, which is \\*\\*not\\*\\* what we want. We can fix it by adding `LARGE\\_PRIME` to the result, and applying `% LARGE\\_PRIME` once again. ↩",
      "content_type": "blog",
      "source_url": "https://nilmamano.com/blog/set-and-map-implementations?category=dsa"
    }
  ]
}